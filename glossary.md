# Glossary

- **[apache-spark](#apache-spark)**: An open-source, [distributed analytics engine](#distributed-analytics-engine) designed for large-scale data processing and [machine learning](#machine-learning), renowned for its in-memory computation and scalability across clusters.
- **[big-data](#big-data)**: Extremely large and complex datasets that traditional data processing applications cannot manage efficiently, characterized by high volume, velocity, and variety.
- **[distributed-data-processing-framework](#distributed-data-processing-framework)**: A software framework that enables the processing of data across multiple nodes in a cluster, leveraging parallelism for scalability and performance.
- **[master-slave-architecture](#master-slave-architecture)**: A system design where a central master node coordinates and distributes tasks to multiple slave (worker) nodes for parallel execution.
- **[driver](#driver)**: The primary process in a Spark application that runs the main program, manages the [sparkcontext](#sparkcontext), and coordinates task execution across [executors](#executors).
- **[sparkcontext](#sparkcontext)**: The entry point and connection hub in a Spark application, facilitating interaction with the cluster and managing [rdds](#rdds), [accumulators](#accumulators), and [broadcast-variables](#broadcast-variables).
- **[logical-execution-plan](#logical-execution-plan)**: A high-level blueprint of computations derived from user code, optimized by the [driver](#driver) before execution.
- **[jobs](#jobs)**: High-level units of work in Spark, initiated by [actions](#actions) and broken into [stages](#stages) and [tasks](#tasks) for execution.
- **[stages](#stages)**: Segments of a [jobs](#jobs) defined by the [dag-scheduler](#dag-scheduler), grouped by data dependencies and [shuffle-boundaries](#shuffle-boundaries).
- **[tasks](#tasks)**: The smallest executable units in Spark, assigned to [executors](#executors) to process individual data partitions.
- **[executors](#executors)**: Worker processes on cluster nodes that execute [tasks](#tasks), store data, and report results back to the [driver](#driver).
- **[fault-tolerance](#fault-tolerance)**: The capability of Spark to recover from failures by recomputing lost data using [lineage-information](#lineage-information).
- **[cluster-manager](#cluster-manager)**: A system that allocates and manages computational resources (CPU, memory) across a cluster for Spark applications.
- **[distributed-collections](#distributed-collections)**: Data structures like [rdds](#rdds) partitioned across a cluster for parallel processing.
- **[rdds](#rdds)**: Resilient Distributed Datasets, immutable collections partitioned across a cluster, providing [fault-tolerance](#fault-tolerance) through [lineage-information](#lineage-information).
- **[accumulators](#accumulators)**: Shared variables in Spark that aggregate values across [executors](#executors), commonly used for counters or sums.
- **[broadcast-variables](#broadcast-variables)**: Read-only variables cached on each node to reduce data transfer overhead during task execution.
- **[standalone-cluster-manager](#standalone-cluster-manager)**: Spark’s built-in resource manager for simple cluster setups, requiring minimal configuration.
- **[apache-mesos](#apache-mesos)**: A versatile cluster manager supporting multiple frameworks, including Spark, for resource allocation.
- **[hadoop-yarn](#hadoop-yarn)**: A resource manager in the Hadoop ecosystem, integrated with Spark for managing cluster resources.
- **[kubernetes](#kubernetes)**: A container orchestration platform used to deploy and manage Spark applications in a containerized environment.
- **[directed-acyclic-graph-dag](#directed-acyclic-graph-dag)**: A directed graph without cycles, representing the sequence of [transformations](#transformations) in Spark for optimized execution.
- **[transformations](#transformations)**: Lazy operations on [rdds](#rdds) that create new RDDs (e.g., map, filter) without immediate computation.
- **[actions](#actions)**: Operations that trigger computation on [rdds](#rdds) and return results to the [driver](#driver) (e.g., collect, count).
- **[dag-scheduler](#dag-scheduler)**: A Spark component that divides the [directed-acyclic-graph-dag](#directed-acyclic-graph-dag) into [stages](#stages) based on data dependencies.
- **[task-scheduler](#task-scheduler)**: A component that assigns [tasks](#tasks) to [executors](#executors) for parallel execution.
- **[shuffle-boundaries](#shuffle-boundaries)**: Points in a Spark job where data is redistributed across nodes, marking the separation of [stages](#stages).
- **[in-memory-computation](#in-memory-computation)**: Spark’s technique of processing data in RAM to enhance speed by reducing disk I/O.
- **[data-caching](#data-caching)**: The practice of storing frequently accessed data in memory to improve performance in iterative processes.
- **[machine-learning](#machine-learning)**: A field of study that develops algorithms to learn from data, supported by Spark’s [mllib](#mllib).
- **[stage-pipelining](#stage-pipelining)**: The execution of multiple operations within a [stages](#stages) without shuffling, minimizing disk I/O.
- **[lineage-information](#lineage-information)**: Metadata tracking the sequence of [transformations](#transformations) on [rdds](#rdds), enabling recomputation after failures.
- **[spark-core](#spark-core)**: The foundational layer of Spark, managing [job-scheduling](#job-scheduling), memory, [fault-recovery](#fault-recovery), and task dispatching.
- **[spark-sql](#spark-sql)**: A module for structured data processing, enabling SQL and Hive Query Language queries on various data sources.
- **[spark-streaming](#spark-streaming)**: A component for real-time data processing using micro-batch streaming from sources like [kafka](#kafka).
- **[mllib](#mllib)**: Spark’s scalable library for [machine-learning](#machine-learning), offering algorithms for classification, regression, and more.
- **[graphx](#graphx)**: A Spark library for graph processing and computation on [distributed-datasets](#distributed-datasets).
- **[kafka](#kafka)**: A distributed streaming platform used as a data source for [spark-streaming](#spark-streaming).
- **[distributed-datasets](#distributed-datasets)**: Data partitioned across a cluster for parallel processing, exemplified by [rdds](#rdds).
- **[distributed-processing](#distributed-processing)**: The execution of computations across multiple nodes to handle large-scale data efficiently.
- **[job-scheduling](#job-scheduling)**: The process of organizing and prioritizing [jobs](#jobs) for execution in Spark.
- **[fault-recovery](#fault-recovery)**: The process of restoring data and operations after failures using [lineage-information](#lineage-information).
- **[fault-handling](#fault-handling)**: The management and mitigation of failures in Spark, including task retries and resource reallocation.
- **[pipeline-execution](#pipeline-execution)**: The sequential execution of operations within a [stages](#stages) to optimize performance.
- **[distributed-analytics-engine](#distributed-analytics-engine)**: A system like Spark that performs analytical computations across distributed nodes for large datasets.
- **[dataframes](#dataframes)**: A higher-level, structured API in Spark built on [rdds](#rdds), optimized for data manipulation and SQL queries.