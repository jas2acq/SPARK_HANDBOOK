{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Handbook\n",
    "## Apache Spark: A Comprehensive Guide for Data Engineers\n",
    "\n",
    "This handbook provides a comprehensive overview of [Apache Spark](#apache-spark), a powerful [distributed data processing framework](#distributed-data-processing-framework) designed for handling [big data](#big-data) workloads with speed, ease of use, and flexibility.\n",
    "\n",
    "## Table of Contents\n",
    "- [1. What is Apache Spark?](#1-what-is-apache-spark)\n",
    "- [2. How Apache Spark Works](#2-how-apache-spark-works)\n",
    "  - [2.1. Key Concepts in Spark’s Operation](#21-key-concepts-in-sparks-operation)\n",
    "  - [2.2. Spark’s Execution Workflow](#22-sparks-execution-workflow)\n",
    "  - [2.3. Performance Optimizations in Spark](#23-performance-optimizations-in-spark)\n",
    "  - [2.4. Spark Workloads and Ecosystem Components](#24-spark-workloads-and-ecosystem-components)\n",
    "- [3. Apache Spark Architecture](#3-apache-spark-architecture)\n",
    "  - [3.1. Driver Program (Master Node)](#31-driver-program-master-node)\n",
    "  - [3.2. Cluster Manager](#32-cluster-manager)\n",
    "  - [3.3. Executors (Worker Nodes)](#33-executors-worker-nodes)\n",
    "  - [3.4. Resilient Distributed Dataset (RDD) - The Core Abstraction](#34-resilient-distributed-dataset-rdd-the-core-abstraction)\n",
    "  - [3.5. Directed Acyclic Graph (DAG)](#35-directed-acyclic-graph-dag)\n",
    "  - [3.6. Execution Modes in Spark](#36-execution-modes-in-spark)\n",
    "    - [3.6.1. Cluster Mode](#361-cluster-mode)\n",
    "    - [3.6.2. Client Mode](#362-client-mode)\n",
    "    - [3.6.3. Local Mode](#363-local-mode)\n",
    "- [4. Main Abstractions of Apache Spark](#4-main-abstractions-of-apache-spark)\n",
    "  - [4.1. Resilient Distributed Datasets (RDDs)](#41-resilient-distributed-datasets-rdds)\n",
    "  - [4.2. Directed Acyclic Graph (DAG)](#42-directed-acyclic-graph-dag)\n",
    "- [5. Spark Core Components and Libraries](#5-spark-core-components-and-libraries)\n",
    "  - [5.1. Spark SQL](#51-spark-sql)\n",
    "  - [5.2. MLlib](#52-mllib)\n",
    "- [6. Why Do Data Engineers Need Spark?](#6-why-do-data-engineers-need-spark)\n",
    "- [7. Typical Use Cases](#7-typical-use-cases)\n",
    "- [8. RDDs and DataFrames in Apache Spark](#8-rdds-and-dataframes-in-apache-spark)\n",
    "  - [8.1. Introduction](#81-introduction)\n",
    "  - [8.2. RDD: Resilient Distributed Dataset](#82-rdd-resilient-distributed-dataset)\n",
    "    - [8.2.1. What is an RDD?](#821-what-is-an-rdd)\n",
    "    - [8.2.2. Key Features](#822-key-features)\n",
    "    - [8.2.3. Creating or Loading Data into an RDD](#823-creating-or-loading-data-into-an-rdd)\n",
    "    - [8.2.4. RDD Transformation and Actions](#824-rdd-transformation-and-actions)\n",
    "  - [8.3. DataFrames](#83-dataframes)\n",
    "    - [8.3.1. What is a DataFrame?](#831-what-is-a-dataframe)\n",
    "    - [8.3.2. Key Features](#832-key-features)\n",
    "    - [8.3.3. Creating or Loading Data into a DataFrame](#833-creating-or-loading-data-into-a-dataframe)\n",
    "    - [8.3.4. Common DataFrame Operations](#834-common-dataframe-operations)\n",
    "  - [8.4. Conversion Between RDD and DataFrame](#84-conversion-between-rdd-and-dataframe)\n",
    "  - [8.5. RDD vs. DataFrame - Comparison](#85-rdd-vs-dataframe-comparison)\n",
    "  - [8.6. Use Case Summary](#86-use-case-summary)\n",
    "  - [8.7. Conclusion](#87-conclusion)\n",
    "- [9. Apache Spark Local Setup](#9-apache-spark-local-setup)\n",
    "  - [9.1. Installing Spark Locally (Native Installation)](#91-installing-spark-locally-native-installation)\n",
    "    - [9.1.1. Prerequisites](#911-prerequisites)\n",
    "    - [9.1.2. Download and Install Spark](#912-download-and-install-spark)\n",
    "    - [9.1.3. Set Environment Variables](#913-set-environment-variables)\n",
    "    - [9.1.4. Install Required Python Libraries](#914-install-required-python-libraries)\n",
    "    - [9.1.5. Test Your Installation](#915-test-your-installation)\n",
    "  - [9.2. Using Docker to Set Up Spark](#92-using-docker-to-set-up-spark)\n",
    "    - [9.2.1. Prerequisites](#921-prerequisites)\n",
    "    - [9.2.2. Standalone Setup](#922-standalone-setup)\n",
    "      - [9.2.2.1. Pull a Spark Docker Image](#9221-pull-a-spark-docker-image)\n",
    "      - [9.2.2.2. Run a Spark Container](#9222-run-a-spark-container)\n",
    "    - [9.2.3. Set Up Spark Cluster](#923-set-up-spark-cluster)\n",
    "      - [9.2.3.1. Start the Cluster](#9231-start-the-cluster)\n",
    "      - [9.2.3.2. Access the Spark Web UI](#9232-access-the-spark-web-ui)\n",
    "      - [9.2.3.3. Submit Jobs](#9233-submit-jobs)\n",
    "      - [9.2.3.4. Setting Up Jupyter Notebook Container for Spark (Optional)](#9234-setting-up-jupyter-notebook-container-for-spark-optional)\n",
    "      - [9.2.3.5. Test Notebook Code](#9235-test-notebook-code)\n",
    "- [10. Extracting and Transforming Data with Apache Spark](#10-extracting-and-transforming-data-with-apache-spark)\n",
    "  - [10.1. Extracting Data with Spark](#101-extracting-data-with-spark)\n",
    "    - [10.1.1. Overview](#1011-overview)\n",
    "    - [10.1.2. Supported Data Sources](#1012-supported-data-sources)\n",
    "    - [10.1.3. Reading Data](#1013-reading-data)\n",
    "    - [10.1.4. Key Options](#1014-key-options)\n",
    "    - [10.1.5. Scenario: Extracting Data from JSON, CSV, and Parquet](#1015-scenario-extracting-data-from-json-csv-and-parquet)\n",
    "  - [10.2. Transforming Data with Spark](#102-transforming-data-with-spark)\n",
    "    - [10.2.1. Overview](#1021-overview)\n",
    "    - [10.2.2. Common Transformation Operations](#1022-common-transformation-operations)\n",
    "    - [10.2.3. Scenario: Joining and Aggregating Sales Data](#1023-scenario-joining-and-aggregating-sales-data)\n",
    "  - [10.3. Best Practices](#103-best-practices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Apache Spark?\n",
    "\n",
    "[Apache Spark](#apache-spark) is an open-source, [distributed analytics engine](#distributed-analytics-engine) designed for large-scale data processing and [machine learning](#machine-learning). It is renowned for its speed, versatility, and ability to scale from a single machine to large clusters of computers. Spark offers APIs in several popular languages, including Python (using PySpark), Scala, Java, and R, making it accessible to a wide audience of data professionals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How Apache Spark Works\n",
    "\n",
    "\n",
    "![How Spark Works](img/how_spark_works.png)\n",
    "\n",
    "[Apache Spark](#apache-spark) is a [distributed data processing framework](#distributed-data-processing-framework) designed to handle [big data](#big-data) workloads with speed, ease of use, and flexibility. The fundamental principle behind Spark's operation is its [master-slave architecture](#master-slave-architecture), which allows it to execute tasks in parallel across a cluster of machines.\n",
    "\n",
    "### 2.1. Key Concepts in Spark’s Operation\n",
    "\n",
    "#### 2.1.1. Driver Program\n",
    "The [driver](#driver) is the central coordinator and controller of a Spark application. When you start a Spark application, the driver runs your main program. It is responsible for:\n",
    "- Creating a [SparkContext](#sparkcontext), which is the entry point to all Spark functionalities.\n",
    "- Converting the user’s code (written in Scala, Python, Java, or R) into a [logical execution plan](#logical-execution-plan).\n",
    "- Breaking down the application into smaller pieces called [jobs](#jobs) and subsequently into [stages](#stages) and [tasks](#tasks).\n",
    "- Scheduling tasks on [executors](#executors) and managing their lifecycle.\n",
    "- Handling [fault tolerance](#fault-tolerance) by retrying failed tasks and reallocating resources.\n",
    "\n",
    "#### 2.1.2. SparkContext\n",
    "[SparkContext](#sparkcontext) represents the connection to the computing cluster. It acts as the interface between your Spark application and the [cluster manager](#cluster-manager), letting your program create [distributed collections](#distributed-collections) ([RDDs](#rdds) — Resilient Distributed Datasets), [accumulators](#accumulators), and [broadcast variables](#broadcast-variables).\n",
    "\n",
    "#### 2.1.3. Executors\n",
    "[Executors](#executors) are worker processes that run on cluster nodes. Each executor:\n",
    "- Receives [tasks](#tasks) from the [driver](#driver).\n",
    "- Executes those tasks concurrently.\n",
    "- Stores intermediate data and results either in memory or on disk.\n",
    "- Returns results and task status (success or failure) back to the driver.\n",
    "Their lifespan is tied to the lifecycle of the Spark application.\n",
    "\n",
    "#### 2.1.4. Cluster Manager\n",
    "The [cluster manager](#cluster-manager) is a separate system responsible for managing cluster resources and allocating them to various applications. Spark can operate with several cluster managers:\n",
    "- [Standalone cluster manager](#standalone-cluster-manager) (provided by Spark itself for simple setups).\n",
    "- [Apache Mesos](#apache-mesos) (a general-purpose cluster manager).\n",
    "- [Hadoop YARN](#hadoop-yarn) (resource manager used with Hadoop clusters).\n",
    "- [Kubernetes](#kubernetes) (for container orchestration).\n",
    "\n",
    "The cluster manager launches the Spark [driver](#driver) and [executors](#executors) on cluster nodes, depending on the execution mode.\n",
    "\n",
    "### 2.2. Spark’s Execution Workflow\n",
    "1. When an application starts, the [driver program](#driver) is launched, which creates the [SparkContext](#sparkcontext).\n",
    "2. The driver creates a [Directed Acyclic Graph (DAG)](#directed-acyclic-graph-dag) representing the computation flow based on user operations ([transformations](#transformations) and [actions](#actions)).\n",
    "3. The [DAG Scheduler](#dag-scheduler) breaks this DAG into [stages](#stages), grouping tasks based on [shuffle boundaries](#shuffle-boundaries) and data dependencies.\n",
    "4. The [Task Scheduler](#task-scheduler) then schedules individual [tasks](#tasks) within the stages for execution on the [executors](#executors).\n",
    "5. Tasks are assigned to executors running on worker nodes.\n",
    "6. Executors perform computations, cache data as needed (to speed up repeated data processing), and report results and status back to the driver.\n",
    "7. The driver aggregates results and completes the [job](#jobs).\n",
    "\n",
    "### 2.3. Performance Optimizations in Spark\n",
    "- [In-Memory Computation](#in-memory-computation): Spark loads data into memory and performs computations there, minimizing slower disk I/O operations.\n",
    "- [Data Caching](#data-caching): Frequently used datasets can be cached in memory across iterations to enhance performance, particularly useful for [machine learning](#machine-learning) and iterative algorithms.\n",
    "- [Stage Pipelining](#stage-pipelining): Multiple operations can be pipelined within a stage if they do not require a shuffle, avoiding unnecessary disk writes.\n",
    "- [Fault Tolerance](#fault-tolerance): Spark maintains [lineage information](#lineage-information) of [RDDs](#rdds), so it knows how to recompute lost data partitions in case of executor failures.\n",
    "\n",
    "### 2.4. Spark Workloads and Ecosystem Components\n",
    "Spark is not just a batch processing engine but a full ecosystem for diverse workloads:\n",
    "- [Spark Core](#spark-core): Handles basic operations like [job scheduling](#job-scheduling), memory management, [fault recovery](#fault-recovery), and task dispatching.\n",
    "- [Spark SQL](#spark-sql): Provides interactive querying capabilities using SQL or Hive Query Language with high-performance engines.\n",
    "- [Spark Streaming](#spark-streaming): Enables real-time data processing through micro-batch streaming of live data sources such as [Kafka](#kafka) and Twitter.\n",
    "- [MLlib](#mllib): Spark’s machine learning library providing scalable algorithms including classification, regression, clustering, and collaborative filtering.\n",
    "- [GraphX](#graphx): Framework for graph processing and computation across [distributed datasets](#distributed-datasets).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apache Spark Architecture\n",
    "\n",
    "[Apache Spark](#apache-spark)’s architecture comprises several key components and follows a modular, layered design optimized for [distributed processing](#distributed-processing).\n",
    "\n",
    "![Spark Architecture](img/spark_architecture.png)\n",
    "\n",
    "### 3.1. Driver Program (Master Node)\n",
    "- Runs your application containing the user’s code.\n",
    "- Manages [SparkContext](#sparkcontext) and coordinates the execution of [tasks](#tasks).\n",
    "- Converts user operations into a [DAG](#directed-acyclic-graph-dag).\n",
    "- Interacts with the [cluster manager](#cluster-manager) for resource allocation.\n",
    "- Oversees [job scheduling](#job-scheduling) via the [DAG Scheduler](#dag-scheduler) and [Task Scheduler](#task-scheduler).\n",
    "- Maintains cluster state and tracks job progress and [fault handling](#fault-handling).\n",
    "\n",
    "The [driver](#driver) is critical because it manages job orchestration and monitors system health and task execution.\n",
    "\n",
    "### 3.2. Cluster Manager\n",
    "- A standalone service or integration with other cluster management tools.\n",
    "- Manages resources across the cluster.\n",
    "- Launches the [driver](#driver) and [executor](#executors) processes as per the requested resources.\n",
    "- Monitors node health and manages failures within the cluster.\n",
    "\n",
    "Supported cluster managers include:\n",
    "- [Spark Standalone](#standalone-cluster-manager) (simple and easy setup).\n",
    "- [Apache Mesos](#apache-mesos) (multi-framework support).\n",
    "- [Hadoop YARN](#hadoop-yarn) (common in Hadoop ecosystems).\n",
    "- [Kubernetes](#kubernetes) (for containerized Spark deployments).\n",
    "\n",
    "### 3.3. Executors (Worker Nodes)\n",
    "- Executor processes run on each worker node in the cluster.\n",
    "- Perform actual data processing by executing [tasks](#tasks) assigned by the [driver](#driver).\n",
    "- Cache data in memory or on disk for efficient reuse.\n",
    "- Handle communication with the driver, sending back task execution results.\n",
    "- Their number can be configured based on workload and cluster size.\n",
    "\n",
    "### 3.4. Resilient Distributed Dataset (RDD) - The Core Abstraction\n",
    "- [RDDs](#rdds) are immutable distributed collections of objects partitioned across the cluster.\n",
    "- They provide [fault tolerance](#fault-tolerance) by logging [lineage information](#lineage-information), enabling automatic recomputation.\n",
    "- Users can perform [transformations](#transformations) (lazy evaluated) and [actions](#actions) on RDDs.\n",
    "- RDD abstractions facilitate parallel computations without explicit data movement handling.\n",
    "\n",
    "### 3.5. Directed Acyclic Graph (DAG)\n",
    "- The [DAG](#directed-acyclic-graph-dag) abstraction represents [stages](#stages) and [tasks](#tasks) of computation.\n",
    "- Directed graph with no cycles that represents the dependencies between [transformations](#transformations).\n",
    "- The [DAG Scheduler](#dag-scheduler) converts the program's DAG into stages for execution optimization.\n",
    "- Enables [pipeline execution](#pipeline-execution) within stages and minimizes overhead of disk I/O.\n",
    "\n",
    "### 3.6. Execution Modes in Spark\n",
    "Spark supports three main modes of execution which influence where the [driver](#driver) and [executors](#executors) run:\n",
    "\n",
    "#### 3.6.1. Cluster Mode\n",
    "- [Driver](#driver) runs inside the cluster on one of the worker nodes.\n",
    "- [Cluster manager](#cluster-manager) manages driver and all executor processes.\n",
    "- Suitable for production deployments.\n",
    "\n",
    "#### 3.6.2. Client Mode\n",
    "- [Driver](#driver) runs on the client machine from which the job was submitted.\n",
    "- [Executors](#executors) run on the cluster nodes.\n",
    "- Useful for interactive debugging or testing.\n",
    "\n",
    "#### 3.6.3. Local Mode\n",
    "- Entire Spark application executes on a single machine.\n",
    "- Parallelism is achieved using multiple threads.\n",
    "- Mostly used for development, experimentation, and debugging.\n",
    "- Not recommended for production jobs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Abstractions of Apache Spark\n",
    "\n",
    "### 4.1. Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "[RDDs](#rdds) are the fundamental data structure in Spark. They represent immutable, [distributed collections](#distributed-collections) of objects partitioned across the cluster. RDDs support two types of operations:\n",
    "\n",
    "- **[Transformations](#transformations):** Lazy operations that define a new RDD from an existing one (e.g., map, filter).\n",
    "- **[Actions](#actions):** Operations that trigger computation and return results (e.g., collect, count).\n",
    "\n",
    "RDDs enable [fault tolerance](#fault-tolerance) by tracking [lineage information](#lineage-information), allowing the system to recompute lost data partitions in case of node failure.\n",
    "\n",
    "### 4.2. Directed Acyclic Graph (DAG)\n",
    "\n",
    "[Apache Spark](#apache-spark) uses a [DAG](#directed-acyclic-graph-dag) to represent the sequence of [transformations](#transformations) applied to [RDDs](#rdds). When a [job](#jobs) is submitted, Spark’s [DAG Scheduler](#dag-scheduler) breaks the computation into [stages](#stages) of [tasks](#tasks) that can be executed in parallel. This [DAG](#directed-acyclic-graph-dag)-based execution plan enables optimization and efficient [job scheduling](#job-scheduling).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spark Core Components and Libraries\n",
    "\n",
    "### 5.1. Spark SQL\n",
    "\n",
    "[Spark SQL](#spark-sql) is Spark’s module for working with structured data. It allows querying data using:\n",
    "- Standard SQL.\n",
    "- Hive Query Language (HQL).\n",
    "- Support for numerous data sources including Hive tables, Parquet, and JSON.\n",
    "\n",
    "[Spark SQL](#spark-sql) integrates SQL queries with Spark’s programmatic APIs ([RDDs](#rdds), [DataFrames](#dataframes)) in Python, Scala, and Java. This tight integration supports complex analytics and interactive querying within a unified application framework.\n",
    "\n",
    "### 5.2. MLlib\n",
    "\n",
    "[MLlib](#mllib) is Spark’s scalable [machine learning](#machine-learning) library. It provides:\n",
    "- Algorithms for classification, regression, clustering, and collaborative filtering.\n",
    "- Utilities for model evaluation and data import.\n",
    "- Low-level primitives such as a generic gradient descent optimization algorithm.\n",
    "\n",
    "### 5.3. GraphX\n",
    "\n",
    "GraphX is Spark’s graph processing library, enabling:\n",
    "- Creation and manipulation of graphs with properties on vertices and edges.\n",
    "- Graph-parallel computations like PageRank and triangle counting.\n",
    "- Operators such as subgraph extraction and vertex mapping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Why Do Data Engineers Need Spark?\n",
    "\n",
    "### 1. Speed and Performance\n",
    "- Spark performs in-memory computing, reducing costly disk read/write operations.\n",
    "- It can be up to 100× faster than Hadoop MapReduce for iterative and interactive workloads.\n",
    "\n",
    "### 2. Scalability\n",
    "- Spark scales from a single machine to thousands of cluster nodes.\n",
    "- Handles petabyte-scale data through distributed processing.\n",
    "\n",
    "### 3. Unified Processing Engine\n",
    "- Supports batch processing, real-time streaming, SQL querying, machine learning, and graph analytics all within one platform.\n",
    "\n",
    "### 4. Language Flexibility and Ease of Use\n",
    "- Provides APIs in Python, Scala, Java, and R.\n",
    "- High-level abstractions (RDDs, DataFrames, Datasets) simplify complex data transformations.\n",
    "\n",
    "### 5. Ecosystem and Integration\n",
    "- Integrates with Hadoop HDFS, Amazon S3, Apache Kafka, and other platforms.\n",
    "- Supports multiple cluster managers for flexible deployment.\n",
    "\n",
    "### 6. Essential for Modern Workloads\n",
    "- Enables ETL pipelines, real-time dashboards, machine learning workflows, and large-scale interactive queries.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Typical Use Cases\n",
    "\n",
    "[Apache Spark](#apache-spark) is widely used for:\n",
    "- **ETL Processes**: Extracting, transforming, and loading data from various sources like CSV, JSON, and Parquet.\n",
    "- **Real-Time Analytics**: Processing streaming data from sources like [Kafka](#kafka) for live dashboards.\n",
    "- **Machine Learning**: Training models on large datasets using [MLlib](#mllib).\n",
    "- **Graph Analytics**: Analyzing relationships in data with [GraphX](#graphx).\n",
    "\n",
    "These use cases are exemplified in the fictional \"Global Gadgets\" dataset, where Spark integrates customer, product, and order data for business intelligence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RDDs and DataFrames in Apache Spark\n",
    "\n",
    "Dataset: `./data/customers.csv`\n",
    "\n",
    "### 8.1. Introduction\n",
    "Apache Spark has two core abstractions for working with distributed data:\n",
    "- **[RDD (Resilient Distributed Dataset)](#rdds):** The original low-level distributed data structure.\n",
    "- **[DataFrame](#dataframes):** A high-level abstraction built on top of RDDs, offering a tabular data structure similar to a database table or Pandas DataFrame.\n",
    "\n",
    "### 8.2. RDD: Resilient Distributed Dataset\n",
    "\n",
    "#### 8.2.1. What is an RDD?\n",
    "An [RDD](#rdds) is an immutable [distributed collection](#distributed-collections) of objects that can be processed in parallel.\n",
    "\n",
    "#### 8.2.2. Key Features\n",
    "- [Fault-tolerant](#fault-tolerance)\n",
    "- Lazy evaluation\n",
    "- Supports [transformations](#transformations) (e.g., `map`, `filter`) and [actions](#actions) (e.g., `collect`, `count`)\n",
    "- Type-safe (in Scala/Java)\n",
    "- No built-in schema\n",
    "\n",
    "#### 8.2.3. Creating or Loading Data into an RDD\n",
    "\n",
    "##### Creating an RDD (PySpark):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RDDExample\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading Data into an RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file (skip header)\n",
    "rdd = sc.textFile(\"./data/customers.csv\")\n",
    "header = rdd.first()\n",
    "rdd_data = rdd.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.4. RDD Transformation and Actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split CSV into fields\n",
    "customers_rdd = rdd_data.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Extract customer names\n",
    "names = customers_rdd.map(lambda x: f\"{x[1]} {x[2]}\").collect()\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3. DataFrames\n",
    "\n",
    "#### 8.3.1. What is a DataFrame?\n",
    "A [DataFrame](#dataframes) is a higher-level abstraction built on [RDDs](#rdds), providing a structured API similar to pandas DataFrames, optimized for performance with [Spark SQL](#spark-sql).\n",
    "\n",
    "#### 8.3.2. Key Features\n",
    "- Schema-aware\n",
    "- Optimized with Catalyst optimizer\n",
    "- Supports SQL-like operations\n",
    "- Integration with various data sources (CSV, JSON, Parquet)\n",
    "\n",
    "#### 8.3.3. Creating or Loading Data into a DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV into DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"./data/customers.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.4. Common DataFrame Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "df.select(\"first_name\", \"email\").show()\n",
    "\n",
    "# Filter customers with missing join dates\n",
    "df.filter(df.join_date.isNull()).show()\n",
    "\n",
    "# Count customers who joined\n",
    "df.filter(df.join_date.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4. Conversion Between RDD and DataFrame\n",
    "\n",
    "#### From RDD to DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Convert RDD to Row RDD\n",
    "row_rdd = customers_rdd.map(lambda x: Row(\n",
    "    customer_id=int(x[0]),\n",
    "    first_name=x[1],\n",
    "    last_name=x[2],\n",
    "    email=x[3],\n",
    "    join_date=x[4] if x[4] != \"\" else None\n",
    "))\n",
    "\n",
    "df_from_rdd = spark.createDataFrame(row_rdd)\n",
    "df_from_rdd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From DataFrame to RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_from_df = df.rdd\n",
    "rdd_from_df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5. RDD vs. DataFrame - Comparison\n",
    "\n",
    "| Feature           | RDD                        | DataFrame               |\n",
    "|-------------------|----------------------------|--------------------------|\n",
    "| Abstraction Level | Low                        | High                    |\n",
    "| API Style         | Functional                 | SQL-like                |\n",
    "| Schema            | Not enforced               | Schema-aware            |\n",
    "| Performance       | Lower                      | Optimized with Catalyst |\n",
    "| Best for          | Custom, fine-grained logic | Queries, aggregations   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6. Use Case Summary\n",
    "\n",
    "| Task                                     | Recommended |\n",
    "|------------------------------------------|-------------|\n",
    "| Load structured CSV data                 | DataFrame   |\n",
    "| Filter or select fields efficiently      | DataFrame   |\n",
    "| Custom parsing, transformation, or logic | RDD         |\n",
    "| SQL-like querying and grouping           | DataFrame   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7. Conclusion\n",
    "\n",
    "- Use [DataFrames](#dataframes) when working with structured data like CSV, JSON, or Parquet.\n",
    "- Use [RDDs](#rdds) when you need custom logic, performance tuning, or low-level transformations.\n",
    "\n",
    "This practical section using your `customers.csv` helps you clearly see how both abstractions work and when to use them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Apache Spark Local Setup\n",
    "\n",
    "In this section, we'll cover two common ways to set up [Apache Spark](#apache-spark) on a local development machine:\n",
    "\n",
    "1. **Installing Spark Locally (Native Installation)**\n",
    "2. **Using Docker to Set Up Spark**\n",
    "\n",
    "### 9.1. Installing Spark Locally (Native Installation)\n",
    "\n",
    "This method involves manually installing Spark and its dependencies on your machine.\n",
    "\n",
    "#### 9.1.1. Prerequisites\n",
    "- **Java (JDK 8 or 11):** [Apache Spark](#apache-spark) runs on the JVM.\n",
    "- **Python 3.x:** Required for PySpark.\n",
    "\n",
    "#### 9.1.2. Download and Install Spark\n",
    "- Download Spark from the [Official Apache Spark website](https://spark.apache.org/downloads.html).\n",
    "  - Choose a version (e.g., Spark 3.4.1) and a pre-built package for Hadoop (e.g., \"Pre-built for Apache Hadoop 3.3 and later\").\n",
    "\n",
    "##### Extract the archive to a directory of your choice\n",
    "\n",
    "**On Linux:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "tar -xzf spark-3.4.1-bin-hadoop3.tgz -C /path/to/your/directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On Windows:**\n",
    "- Use a tool like 7-Zip or WinRAR.\n",
    "  - Right-click the downloaded `.tgz` file.\n",
    "  - Select \"Extract Here\" or \"Extract to spark-3.4.1-bin-hadoop3\".\n",
    "  - Move the extracted folder to your desired location.\n",
    "\n",
    "#### 9.1.3. Set Environment Variables\n",
    "Set the following environment variables so your system can find Spark and Java.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Linux (add to ~/.bashrc or ~/.zshrc)\n",
    "export SPARK_HOME=/path/to/your/directory/spark-3.4.1-bin-hadoop3\n",
    "export PATH=$PATH:$SPARK_HOME/bin\n",
    "export JAVA_HOME=/path/to/your/java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Windows, set environment variables via System Properties > Environment Variables.\n",
    "\n",
    "#### 9.1.4. Install Required Python Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyspark findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1.5. Test Your Installation\n",
    "Start the PySpark shell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or test with a small script:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
    "print(spark.range(5).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2. Using Docker to Set Up Spark\n",
    "\n",
    "An alternative way is to run [Apache Spark](#apache-spark) inside Docker containers. This avoids manual setup and ensures a clean environment.\n",
    "\n",
    "#### 9.2.1. Prerequisites\n",
    "- Docker installed on your system ([Install Docker](https://docs.docker.com/get-docker/))\n",
    "\n",
    "#### 9.2.2. Standalone Setup\n",
    "\n",
    "##### 9.2.2.1. Pull a Spark Docker Image\n",
    "You can use an existing image from Docker Hub or customize it using a Dockerfile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker pull bitnami/spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.2.2.2. Run a Spark Container\n",
    "Start a Spark standalone container:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run -it bitnami/spark pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2.3. Set Up Spark Cluster\n",
    "You can create a local Spark cluster with [`docker-compose.yaml`](./docker-compose.yaml).\n",
    "\n",
    "##### 9.2.3.1. Start the Cluster\n",
    "Run the following command to start the cluster:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.2.3.2. Access the Spark Web UI\n",
    "- Master: [http://localhost:8080](http://localhost:8080)\n",
    "- Worker: [http://localhost:8081](http://localhost:8081)\n",
    "\n",
    "##### 9.2.3.3. Submit Jobs\n",
    "You can submit jobs using the spark-submit tool or run a PySpark shell inside the container:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker exec -it spark-master pyspark --master spark://spark-master:7077"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.2.3.4. Setting Up Jupyter Notebook Container for Spark (Optional)\n",
    "\n",
    "Running a Jupyter Notebook container alongside your Spark services is a great way to interactively test Spark code using PySpark.\n",
    "\n",
    "- Uncomment the `jupyter` service block in the [`docker-compose.yaml`](./docker-compose.yaml) file.\n",
    "- Ensure the `notebooks` directory exists in the same location as your `docker-compose.yaml`:\n",
    "    ```bash\n",
    "    mkdir notebooks\n",
    "    ```\n",
    "  This directory will be mounted into the Jupyter container so that your notebooks are saved persistently.\n",
    "\n",
    "- To start the whole cluster (including Jupyter):\n",
    "    ```bash\n",
    "    docker-compose up -d\n",
    "    ```\n",
    "- To start only the Jupyter container (after cluster is running):\n",
    "    ```bash\n",
    "    docker-compose up -d jupyter\n",
    "    ```\n",
    "- You can now access the notebook UI at: [http://localhost:8888](http://localhost:8888)\n",
    "\n",
    "  Use the token shown in the terminal (when the Jupyter container starts) to log in.\n",
    "\n",
    "##### 9.2.3.5. Test Notebook Code\n",
    "\n",
    "In a new notebook, run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NotebookSpark\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.range(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff90d22",
   "metadata": {},
   "source": [
    "## 10. Extracting and Transforming Data with Apache Spark\n",
    "\n",
    "This section provides a comprehensive guide to extracting and transforming data using Apache Spark, focusing on Spark SQL and DataFrame APIs. It includes detailed explanations, practical scenarios, code examples, and best practices to help you master these critical aspects of Spark.\n",
    "\n",
    "### 10.1. Extracting Data with Spark\n",
    "\n",
    "#### 10.1.1. Overview\n",
    "Extracting data in Apache Spark involves loading data from various sources into DataFrames for further processing. Spark’s DataFrame API provides a unified interface to read data from file formats like JSON, CSV, and Parquet, as well as databases and cloud storage.\n",
    "\n",
    "#### 10.1.2. Supported Data Sources\n",
    "Spark supports a wide range of data sources, including:\n",
    "- `File Formats`: CSV, JSON, Parquet, ORC, Avro, Text\n",
    "- `Databases`: JDBC/ODBC (MySQL, PostgreSQL, SQL Server, etc.)\n",
    "- `Big Data Systems`: Hadoop HDFS, Apache Hive, Apache HBase\n",
    "- `Cloud Storage`: AWS S3, Google Cloud Storage, Azure Blob Storage\n",
    "- `Other`: Kafka, NoSQL databases like Cassandra\n",
    "\n",
    "#### 10.1.3. Reading Data\n",
    "Spark provides the `spark.read` API to load data into DataFrames. Common methods include:\n",
    "- `spark.read.csv(path)`: Reads CSV files\n",
    "- `spark.read.json(path)`: Reads JSON files\n",
    "- `spark.read.parquet(path)`: Reads Parquet files\n",
    "- `spark.read.jdbc(url, table, properties)`: Reads from JDBC databases.\n",
    "\n",
    "#### 10.1.4. Key Options\n",
    "- `header=True`: Treats the first row as column names (CSV).\n",
    "- `inferSchema=True`: Automatically infers column data types.\n",
    "- `schema=StructType`: Specifies a custom schema to avoid inference overhead.\n",
    "- `mode`: Controls error handling (permissive, dropmalformed, failfast).\n",
    "\n",
    "#### 10.1.5. Scenario: Extracting Data from JSON, CSV, and Parquet\n",
    "**Problem**: Load the `products.json`, `customers.csv`, and `orders.parquet` datasets into Spark DataFrames, ensuring proper schema handling and error management for missing or inconsistent data.\n",
    "\n",
    "**Solution**: \n",
    "- Define explicit schemas to ensure correct data types.\n",
    "- Handle missing or malformed data during extraction.\n",
    "- Cache DataFrames for repeated use and save them in Parquet for unified storage.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0de231",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkHandbook\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for products.json\n",
    "products_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), False),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for customers.csv\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"join_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Read products.json\n",
    "products_df = spark.read \\\n",
    "    .schema(products_schema) \\\n",
    "    .json(\"./data/products.json\")\n",
    " \n",
    "\n",
    "# Read customers.csv\n",
    "customers_df = spark.read \\\n",
    "    .schema(customers_schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"dropmalformed\") \\\n",
    "    .csv(\"./data/customers.csv\")\n",
    "\n",
    "# Read orders.parquet\n",
    "orders_df = spark.read.parquet(\"./data/orders_parquet/orders.parquet\")\n",
    "\n",
    "# Cache DataFrames for performance\n",
    "products_df.cache()\n",
    "customers_df.cache()\n",
    "orders_df.cache()\n",
    "\n",
    "# Show sample data\n",
    "print(\"Products:\")\n",
    "products_df.show(5, truncate=False)\n",
    "print(\"Customers:\")\n",
    "customers_df.show(5, truncate=False)\n",
    "print(\"Orders:\")\n",
    "orders_df.show(5, truncate=False)\n",
    "\n",
    "# Save to Parquet for unified storage\n",
    "products_df.write.mode(\"overwrite\").parquet(\"./data/cleaned/products_clean.parquet\")\n",
    "customers_df.write.mode(\"overwrite\").parquet(\"./data/cleaned/customers_clean.parquet\")\n",
    "orders_df.write.mode(\"overwrite\").parquet(\"./data/cleaned/orders_clean.parquet\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9399f567",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- Defines explicit schemas to ensure correct data types and avoid inference overhead.\n",
    "- Uses `dropmalformed` mode for the CSV to skip any malformed rows.\n",
    "- Reads `orders.parquet` directly, as Parquet files include schema metadata.\n",
    "- Caches DataFrames to improve performance for subsequent transformations.\n",
    "- Saves cleaned DataFrames to Parquet for efficient storage and querying."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06937d1b",
   "metadata": {},
   "source": [
    "### 10.2. Transforming Data with Spark\n",
    "\n",
    "#### 10.2.1. Overview\n",
    "Transforming data in Spark involves manipulating [DataFrames](#dataframes) to clean, enrich, or aggregate data for analysis. Spark's DataFrame API and SQL queries support operations like joins, aggregations, filtering, and window functions, optimized for [distributed execution](#distributed-processing). The provided datasets will be used to demonstrate these transformations in practical scenarios.\n",
    "\n",
    "#### 10.2.2. Common Transformation Operations\n",
    "\n",
    "**Joins**: <br>\n",
    "Joins combine [DataFrames](#dataframes) based on a key. Common types include:\n",
    "- `Inner Join`: Returns only matching rows.\n",
    "- `Left Outer Join`: Includes all rows from the left DataFrame, with nulls for non-matching rows.\n",
    "- `Right Outer Join`: Includes all rows from the right DataFrame.\n",
    "- `Full Outer Join`: Includes all rows from both DataFrames.\n",
    "\n",
    "**Syntax**:\n",
    "```python \n",
    "result_df = df1.join(df2, df1.key == df2.key, \"inner\")\n",
    "```\n",
    "\n",
    "**Aggregations**: <br>\n",
    "Aggregations summarize data using functions like `sum`, `avg`, `count`, `min`, `max`, typically after `groupBy`\n",
    "\n",
    "**Syntax**: <br>\n",
    "```python \n",
    "agg_df = df.groupBy(\"column\").agg({\"other_column\": \"sum\"})\n",
    "```\n",
    "\n",
    "**Filtering and Selecting**: <br>\n",
    "- **Filtering**: Select rows with filter or where.\n",
    "- **Selecting**: Choose columns with select.\n",
    "\n",
    "**Syntax**:\n",
    "```python \n",
    "filtered_df = df.filter(col(\"column\") > value)\n",
    "selected_df = df.select(\"column1\", \"column2\")\n",
    "```\n",
    "\n",
    "**Handling Missing Data**\n",
    "- `Drop nulls`: df.dropna(subset=[\"column\"])\n",
    "- `Fill nulls`: df.fillna(value, subset=[\"column\"])\n",
    "- `Replace values`: df.replace(old_value, new_value, subset=[\"column\"])\n",
    "\n",
    "#### 10.2.3. Scenario: Joining and Aggregating Sales Data\n",
    "**Problem**:\n",
    "Combine the `products`, `customers`, and `orders` datasets to calculate total sales and average order amount per product category. Standardize the category column in products and handle missing data in customers.\n",
    "\n",
    "**Solution**: <br>\n",
    "- Join the three [DataFrames](#dataframes) on appropriate keys.\n",
    "- Standardize `category` in `products`.\n",
    "- Handle missing `last_name` and `join_date` in customers.\n",
    "- Aggregate by `category` to compute sales metrics.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c5068c",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as sum_, avg, count, coalesce, lit, upper\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SparkTransformation\").getOrCreate()\n",
    "\n",
    "# Load data\n",
    "products_df = spark.read.parquet(\"./data/cleaned/products_clean.parquet\")\n",
    "customers_df = spark.read.parquet(\"./data/cleaned/customers_clean.parquet\")\n",
    "orders_df = spark.read.parquet(\"./data/cleaned/orders_clean.parquet\")\n",
    "\n",
    "# Standardize category in products (convert to uppercase)\n",
    "products_df = products_df.withColumn(\"category\", upper(col(\"category\")))\n",
    "\n",
    "# Handle missing data in customers\n",
    "customers_df = customers_df \\\n",
    "    .withColumn(\"last_name\", coalesce(col(\"last_name\"), lit(\"Unknown\"))) \\\n",
    "    .withColumn(\"join_date\", coalesce(col(\"join_date\"), lit(\"2023-01-01\").cast(DateType())))\n",
    "\n",
    "# Join DataFrames\n",
    "joined_df = orders_df \\\n",
    "    .join(customers_df, \"customer_id\", \"left_outer\") \\\n",
    "    .join(products_df, \"product_id\", \"inner\")\n",
    "\n",
    "# Print joined dataframe\n",
    "print(\"Joined DataFrame:\")\n",
    "joined_df.show(truncate=False)\n",
    "\n",
    "# Aggregate by category\n",
    "summary_df = joined_df.groupBy(\"category\") \\\n",
    "    .agg(\n",
    "        sum_(\"total_price\").alias(\"total_sales\"),\n",
    "        avg(\"total_price\").alias(\"avg_order_amount\"),\n",
    "        count(\"order_id\").alias(\"order_count\")\n",
    "    )\n",
    "\n",
    "# Format numerical columns\n",
    "summary_df = summary_df.select(\n",
    "    col(\"category\"),\n",
    "    col(\"total_sales\").cast(\"decimal(10,2)\"),\n",
    "    col(\"avg_order_amount\").cast(\"decimal(10,2)\"),\n",
    "    col(\"order_count\")\n",
    ")\n",
    "\n",
    "# Save and show results\n",
    "summary_df.write.mode(\"overwrite\").parquet(\"./data/cleaned/sales_summary.parquet\")\n",
    "print(\"Sales Summary:\")\n",
    "summary_df.show(truncate=False)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3228fc5c",
   "metadata": {},
   "source": [
    "**Explanation**: <br>\n",
    "- Standardizes `category` in `products_df` to uppercase to fix any inconsistencies (e.g., “accessories” vs. “Accessories”).\n",
    "- Fills missing `last_name` with “Unknown” and `join_date` with a default date in `customers_df`.\n",
    "- Performs a left outer join for `orders` and `customers` to include all orders, and an inner join with `products` to ensure valid products.\n",
    "- Aggregates by `category`, computing total sales, average order amount, and order count.\n",
    "- Formats numerical columns to `decimal(10,2)` for readability.\n",
    "- Saves results to Parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2bfe1b",
   "metadata": {},
   "source": [
    "### 10.3. Best Practices\n",
    "\n",
    "- **Explicit Schemas**: Always define schemas for JSON and CSV to ensure correct data types and avoid inference costs.\n",
    "- **Handle Inconsistencies**: Standardize case-sensitive fields (e.g., `category`) early in the pipeline.\n",
    "- **Null Handling**: Address missing data before joins or aggregations to prevent unexpected results.\n",
    "- **Join Optimization**: Use inner joins when possible; use left outer joins to preserve data when needed.\n",
    "- **Columnar Storage**: Use Parquet for intermediate and output data to leverage compression and columnar access.\n",
    "- **SQL for Readability**: Use [Spark SQL](#spark-sql) for complex transformations when it improves clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cac0f2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
