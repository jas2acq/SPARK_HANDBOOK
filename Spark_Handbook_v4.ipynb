{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Handbook\n",
    "## Apache Spark: A Comprehensive Guide for Data Engineers\n",
    "\n",
    "This handbook provides a comprehensive overview of [Apache Spark](#apache-spark), a powerful [distributed data processing framework](#distributed-data-processing-framework) designed for handling [big data](#big-data) workloads with speed, ease of use, and flexibility.\n",
    "\n",
    "## Table of Contents\n",
    "- [1. What is Apache Spark?](#1-what-is-apache-spark)\n",
    "- [2. How Apache Spark Works](#2-how-apache-spark-works)\n",
    "  - [2.1. Key Concepts in Spark’s Operation](#21-key-concepts-in-sparks-operation)\n",
    "  - [2.2. Spark’s Execution Workflow](#22-sparks-execution-workflow)\n",
    "  - [2.3. Performance Optimizations in Spark](#23-performance-optimizations-in-spark)\n",
    "  - [2.4. Spark Workloads and Ecosystem Components](#24-spark-workloads-and-ecosystem-components)\n",
    "- [3. Apache Spark Architecture](#3-apache-spark-architecture)\n",
    "  - [3.1. Driver Program (Master Node)](#31-driver-program-master-node)\n",
    "  - [3.2. Cluster Manager](#32-cluster-manager)\n",
    "  - [3.3. Executors (Worker Nodes)](#33-executors-worker-nodes)\n",
    "  - [3.4. Resilient Distributed Dataset (RDD) - The Core Abstraction](#34-resilient-distributed-dataset-rdd-the-core-abstraction)\n",
    "  - [3.5. Directed Acyclic Graph (DAG)](#35-directed-acyclic-graph-dag)\n",
    "  - [3.6. Execution Modes in Spark](#36-execution-modes-in-spark)\n",
    "    - [3.6.1. Cluster Mode](#361-cluster-mode)\n",
    "    - [3.6.2. Client Mode](#362-client-mode)\n",
    "    - [3.6.3. Local Mode](#363-local-mode)\n",
    "- [4. Main Abstractions of Apache Spark](#4-main-abstractions-of-apache-spark)\n",
    "  - [4.1. Resilient Distributed Datasets (RDDs)](#41-resilient-distributed-datasets-rdds)\n",
    "  - [4.2. Directed Acyclic Graph (DAG)](#42-directed-acyclic-graph-dag)\n",
    "- [5. Spark Core Components and Libraries](#5-spark-core-components-and-libraries)\n",
    "  - [5.1. Spark SQL](#51-spark-sql)\n",
    "  - [5.2. MLlib](#52-mllib)\n",
    "- [6. Why Do Data Engineers Need Spark?](#6-why-do-data-engineers-need-spark)\n",
    "- [7. Typical Use Cases](#7-typical-use-cases)\n",
    "- [8. RDDs and DataFrames in Apache Spark](#8-rdds-and-dataframes-in-apache-spark)\n",
    "  - [8.1. Introduction](#81-introduction)\n",
    "  - [8.2. RDD: Resilient Distributed Dataset](#82-rdd-resilient-distributed-dataset)\n",
    "    - [8.2.1. What is an RDD?](#821-what-is-an-rdd)\n",
    "    - [8.2.2. Key Features](#822-key-features)\n",
    "    - [8.2.3. Creating or Loading Data into an RDD](#823-creating-or-loading-data-into-an-rdd)\n",
    "    - [8.2.4. RDD Transformation and Actions](#824-rdd-transformation-and-actions)\n",
    "  - [8.3. DataFrames](#83-dataframes)\n",
    "    - [8.3.1. What is a DataFrame?](#831-what-is-a-dataframe)\n",
    "    - [8.3.2. Key Features](#832-key-features)\n",
    "    - [8.3.3. Creating or Loading Data into a DataFrame](#833-creating-or-loading-data-into-a-dataframe)\n",
    "    - [8.3.4. Common DataFrame Operations](#834-common-dataframe-operations)\n",
    "  - [8.4. Conversion Between RDD and DataFrame](#84-conversion-between-rdd-and-dataframe)\n",
    "  - [8.5. RDD vs. DataFrame - Comparison](#85-rdd-vs-dataframe-comparison)\n",
    "  - [8.6. Use Case Summary](#86-use-case-summary)\n",
    "  - [8.7. Conclusion](#87-conclusion)\n",
    "- [9. Apache Spark Local Setup](#9-apache-spark-local-setup)\n",
    "  - [9.1. Installing Spark Locally (Native Installation)](#91-installing-spark-locally-native-installation)\n",
    "    - [9.1.1. Prerequisites](#911-prerequisites)\n",
    "    - [9.1.2. Download and Install Spark](#912-download-and-install-spark)\n",
    "    - [9.1.3. Set Environment Variables](#913-set-environment-variables)\n",
    "    - [9.1.4. Install Required Python Libraries](#914-install-required-python-libraries)\n",
    "    - [9.1.5. Test Your Installation](#915-test-your-installation)\n",
    "  - [9.2. Using Docker to Set Up Spark](#92-using-docker-to-set-up-spark)\n",
    "    - [9.2.1. Prerequisites](#921-prerequisites)\n",
    "    - [9.2.2. Standalone Setup](#922-standalone-setup)\n",
    "      - [9.2.2.1. Pull a Spark Docker Image](#9221-pull-a-spark-docker-image)\n",
    "      - [9.2.2.2. Run a Spark Container](#9222-run-a-spark-container)\n",
    "    - [9.2.3. Set Up Spark Cluster](#923-set-up-spark-cluster)\n",
    "      - [9.2.3.1. Start the Cluster](#9231-start-the-cluster)\n",
    "      - [9.2.3.2. Access the Spark Web UI](#9232-access-the-spark-web-ui)\n",
    "      - [9.2.3.3. Submit Jobs](#9233-submit-jobs)\n",
    "      - [9.2.3.4. Setting Up Jupyter Notebook Container for Spark (Optional)](#9234-setting-up-jupyter-notebook-container-for-spark-optional)\n",
    "      - [9.2.3.5. Test Notebook Code](#9235-test-notebook-code)\n",
    "\n",
    "[View Spark Architecture Diagram](https://spark.apache.org/docs/latest/img/cluster-overview.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Apache Spark?\n",
    "\n",
    "[Apache Spark](#apache-spark) is an open-source, [distributed analytics engine](#distributed-analytics-engine) designed for large-scale data processing and [machine learning](#machine-learning). It is renowned for its speed, versatility, and ability to scale from a single machine to large clusters of computers. Spark offers APIs in several popular languages, including Python (using PySpark), Scala, Java, and R, making it accessible to a wide audience of data professionals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How Apache Spark Works\n",
    "\n",
    "\n",
    "![How Spark Works](img/how_spark_works.png)\n",
    "\n",
    "[Apache Spark](#apache-spark) is a [distributed data processing framework](#distributed-data-processing-framework) designed to handle [big data](#big-data) workloads with speed, ease of use, and flexibility. The fundamental principle behind Spark's operation is its [master-slave architecture](#master-slave-architecture), which allows it to execute tasks in parallel across a cluster of machines.\n",
    "\n",
    "### 2.1. Key Concepts in Spark’s Operation\n",
    "\n",
    "#### 2.1.1. Driver Program\n",
    "The [driver](#driver) is the central coordinator and controller of a Spark application. When you start a Spark application, the driver runs your main program. It is responsible for:\n",
    "- Creating a [SparkContext](#sparkcontext), which is the entry point to all Spark functionalities.\n",
    "- Converting the user’s code (written in Scala, Python, Java, or R) into a [logical execution plan](#logical-execution-plan).\n",
    "- Breaking down the application into smaller pieces called [jobs](#jobs) and subsequently into [stages](#stages) and [tasks](#tasks).\n",
    "- Scheduling tasks on [executors](#executors) and managing their lifecycle.\n",
    "- Handling [fault tolerance](#fault-tolerance) by retrying failed tasks and reallocating resources.\n",
    "\n",
    "#### 2.1.2. SparkContext\n",
    "[SparkContext](#sparkcontext) represents the connection to the computing cluster. It acts as the interface between your Spark application and the [cluster manager](#cluster-manager), letting your program create [distributed collections](#distributed-collections) ([RDDs](#rdds) — Resilient Distributed Datasets), [accumulators](#accumulators), and [broadcast variables](#broadcast-variables).\n",
    "\n",
    "#### 2.1.3. Executors\n",
    "[Executors](#executors) are worker processes that run on cluster nodes. Each executor:\n",
    "- Receives [tasks](#tasks) from the [driver](#driver).\n",
    "- Executes those tasks concurrently.\n",
    "- Stores intermediate data and results either in memory or on disk.\n",
    "- Returns results and task status (success or failure) back to the driver.\n",
    "Their lifespan is tied to the lifecycle of the Spark application.\n",
    "\n",
    "#### 2.1.4. Cluster Manager\n",
    "The [cluster manager](#cluster-manager) is a separate system responsible for managing cluster resources and allocating them to various applications. Spark can operate with several cluster managers:\n",
    "- [Standalone cluster manager](#standalone-cluster-manager) (provided by Spark itself for simple setups).\n",
    "- [Apache Mesos](#apache-mesos) (a general-purpose cluster manager).\n",
    "- [Hadoop YARN](#hadoop-yarn) (resource manager used with Hadoop clusters).\n",
    "- [Kubernetes](#kubernetes) (for container orchestration).\n",
    "\n",
    "The cluster manager launches the Spark [driver](#driver) and [executors](#executors) on cluster nodes, depending on the execution mode.\n",
    "\n",
    "### 2.2. Spark’s Execution Workflow\n",
    "1. When an application starts, the [driver program](#driver) is launched, which creates the [SparkContext](#sparkcontext).\n",
    "2. The driver creates a [Directed Acyclic Graph (DAG)](#directed-acyclic-graph-dag) representing the computation flow based on user operations ([transformations](#transformations) and [actions](#actions)).\n",
    "3. The [DAG Scheduler](#dag-scheduler) breaks this DAG into [stages](#stages), grouping tasks based on [shuffle boundaries](#shuffle-boundaries) and data dependencies.\n",
    "4. The [Task Scheduler](#task-scheduler) then schedules individual [tasks](#tasks) within the stages for execution on the [executors](#executors).\n",
    "5. Tasks are assigned to executors running on worker nodes.\n",
    "6. Executors perform computations, cache data as needed (to speed up repeated data processing), and report results and status back to the driver.\n",
    "7. The driver aggregates results and completes the [job](#jobs).\n",
    "\n",
    "### 2.3. Performance Optimizations in Spark\n",
    "- [In-Memory Computation](#in-memory-computation): Spark loads data into memory and performs computations there, minimizing slower disk I/O operations.\n",
    "- [Data Caching](#data-caching): Frequently used datasets can be cached in memory across iterations to enhance performance, particularly useful for [machine learning](#machine-learning) and iterative algorithms.\n",
    "- [Stage Pipelining](#stage-pipelining): Multiple operations can be pipelined within a stage if they do not require a shuffle, avoiding unnecessary disk writes.\n",
    "- [Fault Tolerance](#fault-tolerance): Spark maintains [lineage information](#lineage-information) of [RDDs](#rdds), so it knows how to recompute lost data partitions in case of executor failures.\n",
    "\n",
    "### 2.4. Spark Workloads and Ecosystem Components\n",
    "Spark is not just a batch processing engine but a full ecosystem for diverse workloads:\n",
    "- [Spark Core](#spark-core): Handles basic operations like [job scheduling](#job-scheduling), memory management, [fault recovery](#fault-recovery), and task dispatching.\n",
    "- [Spark SQL](#spark-sql): Provides interactive querying capabilities using SQL or Hive Query Language with high-performance engines.\n",
    "- [Spark Streaming](#spark-streaming): Enables real-time data processing through micro-batch streaming of live data sources such as [Kafka](#kafka) and Twitter.\n",
    "- [MLlib](#mllib): Spark’s machine learning library providing scalable algorithms including classification, regression, clustering, and collaborative filtering.\n",
    "- [GraphX](#graphx): Framework for graph processing and computation across [distributed datasets](#distributed-datasets).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apache Spark Architecture\n",
    "\n",
    "[Apache Spark](#apache-spark)’s architecture comprises several key components and follows a modular, layered design optimized for [distributed processing](#distributed-processing).\n",
    "\n",
    "![Spark Architecture](img/spark_architecture.png)\n",
    "\n",
    "### 3.1. Driver Program (Master Node)\n",
    "- Runs your application containing the user’s code.\n",
    "- Manages [SparkContext](#sparkcontext) and coordinates the execution of [tasks](#tasks).\n",
    "- Converts user operations into a [DAG](#directed-acyclic-graph-dag).\n",
    "- Interacts with the [cluster manager](#cluster-manager) for resource allocation.\n",
    "- Oversees [job scheduling](#job-scheduling) via the [DAG Scheduler](#dag-scheduler) and [Task Scheduler](#task-scheduler).\n",
    "- Maintains cluster state and tracks job progress and [fault handling](#fault-handling).\n",
    "\n",
    "The [driver](#driver) is critical because it manages job orchestration and monitors system health and task execution.\n",
    "\n",
    "### 3.2. Cluster Manager\n",
    "- A standalone service or integration with other cluster management tools.\n",
    "- Manages resources across the cluster.\n",
    "- Launches the [driver](#driver) and [executor](#executors) processes as per the requested resources.\n",
    "- Monitors node health and manages failures within the cluster.\n",
    "\n",
    "Supported cluster managers include:\n",
    "- [Spark Standalone](#standalone-cluster-manager) (simple and easy setup).\n",
    "- [Apache Mesos](#apache-mesos) (multi-framework support).\n",
    "- [Hadoop YARN](#hadoop-yarn) (common in Hadoop ecosystems).\n",
    "- [Kubernetes](#kubernetes) (for containerized Spark deployments).\n",
    "\n",
    "### 3.3. Executors (Worker Nodes)\n",
    "- Executor processes run on each worker node in the cluster.\n",
    "- Perform actual data processing by executing [tasks](#tasks) assigned by the [driver](#driver).\n",
    "- Cache data in memory or on disk for efficient reuse.\n",
    "- Handle communication with the driver, sending back task execution results.\n",
    "- Their number can be configured based on workload and cluster size.\n",
    "\n",
    "### 3.4. Resilient Distributed Dataset (RDD) - The Core Abstraction\n",
    "- [RDDs](#rdds) are immutable distributed collections of objects partitioned across the cluster.\n",
    "- They provide [fault tolerance](#fault-tolerance) by logging [lineage information](#lineage-information), enabling automatic recomputation.\n",
    "- Users can perform [transformations](#transformations) (lazy evaluated) and [actions](#actions) on RDDs.\n",
    "- RDD abstractions facilitate parallel computations without explicit data movement handling.\n",
    "\n",
    "### 3.5. Directed Acyclic Graph (DAG)\n",
    "- The [DAG](#directed-acyclic-graph-dag) abstraction represents [stages](#stages) and [tasks](#tasks) of computation.\n",
    "- Directed graph with no cycles that represents the dependencies between [transformations](#transformations).\n",
    "- The [DAG Scheduler](#dag-scheduler) converts the program's DAG into stages for execution optimization.\n",
    "- Enables [pipeline execution](#pipeline-execution) within stages and minimizes overhead of disk I/O.\n",
    "\n",
    "### 3.6. Execution Modes in Spark\n",
    "Spark supports three main modes of execution which influence where the [driver](#driver) and [executors](#executors) run:\n",
    "\n",
    "#### 3.6.1. Cluster Mode\n",
    "- [Driver](#driver) runs inside the cluster on one of the worker nodes.\n",
    "- [Cluster manager](#cluster-manager) manages driver and all executor processes.\n",
    "- Suitable for production deployments.\n",
    "\n",
    "#### 3.6.2. Client Mode\n",
    "- [Driver](#driver) runs on the client machine from which the job was submitted.\n",
    "- [Executors](#executors) run on the cluster nodes.\n",
    "- Useful for interactive debugging or testing.\n",
    "\n",
    "#### 3.6.3. Local Mode\n",
    "- Entire Spark application executes on a single machine.\n",
    "- Parallelism is achieved using multiple threads.\n",
    "- Mostly used for development, experimentation, and debugging.\n",
    "- Not recommended for production jobs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Abstractions of Apache Spark\n",
    "\n",
    "### 4.1. Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "[RDDs](#rdds) are the fundamental data structure in Spark. They represent immutable, [distributed collections](#distributed-collections) of objects partitioned across the cluster. RDDs support two types of operations:\n",
    "\n",
    "- **[Transformations](#transformations):** Lazy operations that define a new RDD from an existing one (e.g., map, filter).\n",
    "- **[Actions](#actions):** Operations that trigger computation and return results (e.g., collect, count).\n",
    "\n",
    "RDDs enable [fault tolerance](#fault-tolerance) by tracking [lineage information](#lineage-information), allowing the system to recompute lost data partitions in case of node failure.\n",
    "\n",
    "### 4.2. Directed Acyclic Graph (DAG)\n",
    "\n",
    "[Apache Spark](#apache-spark) uses a [DAG](#directed-acyclic-graph-dag) to represent the sequence of [transformations](#transformations) applied to [RDDs](#rdds). When a [job](#jobs) is submitted, Spark’s [DAG Scheduler](#dag-scheduler) breaks the computation into [stages](#stages) of [tasks](#tasks) that can be executed in parallel. This [DAG](#directed-acyclic-graph-dag)-based execution plan enables optimization and efficient [job scheduling](#job-scheduling).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spark Core Components and Libraries\n",
    "\n",
    "### 5.1. Spark SQL\n",
    "\n",
    "[Spark SQL](#spark-sql) is Spark’s module for working with structured data. It allows querying data using:\n",
    "- Standard SQL.\n",
    "- Hive Query Language (HQL).\n",
    "- Support for numerous data sources including Hive tables, Parquet, and JSON.\n",
    "\n",
    "[Spark SQL](#spark-sql) integrates SQL queries with Spark’s programmatic APIs ([RDDs](#rdds), [DataFrames](#dataframes)) in Python, Scala, and Java. This tight integration supports complex analytics and interactive querying within a unified application framework.\n",
    "\n",
    "### 5.2. MLlib\n",
    "\n",
    "[MLlib](#mllib) is Spark’s scalable [machine learning](#machine-learning) library. It provides:\n",
    "- Algorithms for classification, regression, clustering, and collaborative filtering.\n",
    "- Utilities for model evaluation and data import.\n",
    "- Low-level primitives such as a generic gradient descent optimization algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Why Do Data Engineers Need Spark?\n",
    "\n",
    "[Apache Spark](#apache-spark) is essential for data engineers due to its ability to handle large-scale [big data](#big-data) processing efficiently. It offers:\n",
    "- **Speed**: In-memory computation accelerates data processing compared to traditional disk-based systems.\n",
    "- **Scalability**: Easily scales from a single node to thousands of nodes in a cluster.\n",
    "- **Ease of Use**: Provides high-level APIs and a unified platform for batch, streaming, and [machine learning](#machine-learning) tasks.\n",
    "- **Fault Tolerance**: Ensures data reliability through [RDD](#rdds) lineage and [fault tolerance](#fault-tolerance) mechanisms.\n",
    "\n",
    "These features make Spark ideal for ETL pipelines, real-time analytics, and large-scale data transformations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Typical Use Cases\n",
    "\n",
    "[Apache Spark](#apache-spark) is widely used for:\n",
    "- **ETL Processes**: Extracting, transforming, and loading data from various sources like CSV, JSON, and Parquet.\n",
    "- **Real-Time Analytics**: Processing streaming data from sources like [Kafka](#kafka) for live dashboards.\n",
    "- **Machine Learning**: Training models on large datasets using [MLlib](#mllib).\n",
    "- **Graph Analytics**: Analyzing relationships in data with [GraphX](#graphx).\n",
    "\n",
    "These use cases are exemplified in the fictional \"Global Gadgets\" dataset, where Spark integrates customer, product, and order data for business intelligence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RDDs and DataFrames in Apache Spark\n",
    "\n",
    "Dataset: `./data/customers.csv`\n",
    "\n",
    "### 8.1. Introduction\n",
    "Apache Spark has two core abstractions for working with distributed data:\n",
    "- **[RDD (Resilient Distributed Dataset)](#rdds):** The original low-level distributed data structure.\n",
    "- **[DataFrame](#dataframes):** A high-level abstraction built on top of RDDs, offering a tabular data structure similar to a database table or Pandas DataFrame.\n",
    "\n",
    "### 8.2. RDD: Resilient Distributed Dataset\n",
    "\n",
    "#### 8.2.1. What is an RDD?\n",
    "An [RDD](#rdds) is an immutable [distributed collection](#distributed-collections) of objects that can be processed in parallel.\n",
    "\n",
    "#### 8.2.2. Key Features\n",
    "- [Fault-tolerant](#fault-tolerance)\n",
    "- Lazy evaluation\n",
    "- Supports [transformations](#transformations) (e.g., `map`, `filter`) and [actions](#actions) (e.g., `collect`, `count`)\n",
    "- Type-safe (in Scala/Java)\n",
    "- No built-in schema\n",
    "\n",
    "#### 8.2.3. Creating or Loading Data into an RDD\n",
    "\n",
    "##### Creating an RDD (PySpark):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RDDExample\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading Data into an RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file (skip header)\n",
    "rdd = sc.textFile(\"./data/customers.csv\")\n",
    "header = rdd.first()\n",
    "rdd_data = rdd.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.4. RDD Transformation and Actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split CSV into fields\n",
    "customers_rdd = rdd_data.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Extract customer names\n",
    "names = customers_rdd.map(lambda x: f\"{x[1]} {x[2]}\").collect()\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3. DataFrames\n",
    "\n",
    "#### 8.3.1. What is a DataFrame?\n",
    "A [DataFrame](#dataframes) is a higher-level abstraction built on [RDDs](#rdds), providing a structured API similar to pandas DataFrames, optimized for performance with [Spark SQL](#spark-sql).\n",
    "\n",
    "#### 8.3.2. Key Features\n",
    "- Schema-aware\n",
    "- Optimized with Catalyst optimizer\n",
    "- Supports SQL-like operations\n",
    "- Integration with various data sources (CSV, JSON, Parquet)\n",
    "\n",
    "#### 8.3.3. Creating or Loading Data into a DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV into DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"./data/customers.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3.4. Common DataFrame Operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "df.select(\"first_name\", \"email\").show()\n",
    "\n",
    "# Filter customers with missing join dates\n",
    "df.filter(df.join_date.isNull()).show()\n",
    "\n",
    "# Count customers who joined\n",
    "df.filter(df.join_date.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4. Conversion Between RDD and DataFrame\n",
    "\n",
    "#### From RDD to DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Convert RDD to Row RDD\n",
    "row_rdd = customers_rdd.map(lambda x: Row(\n",
    "    customer_id=int(x[0]),\n",
    "    first_name=x[1],\n",
    "    last_name=x[2],\n",
    "    email=x[3],\n",
    "    join_date=x[4] if x[4] != \"\" else None\n",
    "))\n",
    "\n",
    "df_from_rdd = spark.createDataFrame(row_rdd)\n",
    "df_from_rdd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From DataFrame to RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_from_df = df.rdd\n",
    "rdd_from_df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5. RDD vs. DataFrame - Comparison\n",
    "\n",
    "| Feature           | RDD                        | DataFrame               |\n",
    "|-------------------|----------------------------|--------------------------|\n",
    "| Abstraction Level | Low                        | High                    |\n",
    "| API Style         | Functional                 | SQL-like                |\n",
    "| Schema            | Not enforced               | Schema-aware            |\n",
    "| Performance       | Lower                      | Optimized with Catalyst |\n",
    "| Best for          | Custom, fine-grained logic | Queries, aggregations   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6. Use Case Summary\n",
    "\n",
    "| Task                                     | Recommended |\n",
    "|------------------------------------------|-------------|\n",
    "| Load structured CSV data                 | DataFrame   |\n",
    "| Filter or select fields efficiently      | DataFrame   |\n",
    "| Custom parsing, transformation, or logic | RDD         |\n",
    "| SQL-like querying and grouping           | DataFrame   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7. Conclusion\n",
    "\n",
    "- Use [DataFrames](#dataframes) when working with structured data like CSV, JSON, or Parquet.\n",
    "- Use [RDDs](#rdds) when you need custom logic, performance tuning, or low-level transformations.\n",
    "\n",
    "This practical section using your `customers.csv` helps you clearly see how both abstractions work and when to use them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Apache Spark Local Setup\n",
    "\n",
    "In this section, we'll cover two common ways to set up [Apache Spark](#apache-spark) on a local development machine:\n",
    "\n",
    "1. **Installing Spark Locally (Native Installation)**\n",
    "2. **Using Docker to Set Up Spark**\n",
    "\n",
    "### 9.1. Installing Spark Locally (Native Installation)\n",
    "\n",
    "This method involves manually installing Spark and its dependencies on your machine.\n",
    "\n",
    "#### 9.1.1. Prerequisites\n",
    "- **Java (JDK 8 or 11):** [Apache Spark](#apache-spark) runs on the JVM.\n",
    "- **Python 3.x:** Required for PySpark.\n",
    "\n",
    "#### 9.1.2. Download and Install Spark\n",
    "- Download Spark from the [Official Apache Spark website](https://spark.apache.org/downloads.html).\n",
    "  - Choose a version (e.g., Spark 3.4.1) and a pre-built package for Hadoop (e.g., \"Pre-built for Apache Hadoop 3.3 and later\").\n",
    "\n",
    "##### Extract the archive to a directory of your choice\n",
    "\n",
    "**On Linux:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "tar -xzf spark-3.4.1-bin-hadoop3.tgz -C /path/to/your/directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On Windows:**\n",
    "- Use a tool like 7-Zip or WinRAR.\n",
    "  - Right-click the downloaded `.tgz` file.\n",
    "  - Select \"Extract Here\" or \"Extract to spark-3.4.1-bin-hadoop3\".\n",
    "  - Move the extracted folder to your desired location.\n",
    "\n",
    "#### 9.1.3. Set Environment Variables\n",
    "Set the following environment variables so your system can find Spark and Java.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Linux (add to ~/.bashrc or ~/.zshrc)\n",
    "export SPARK_HOME=/path/to/your/directory/spark-3.4.1-bin-hadoop3\n",
    "export PATH=$PATH:$SPARK_HOME/bin\n",
    "export JAVA_HOME=/path/to/your/java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Windows, set environment variables via System Properties > Environment Variables.\n",
    "\n",
    "#### 9.1.4. Install Required Python Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyspark findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1.5. Test Your Installation\n",
    "Start the PySpark shell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or test with a small script:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
    "print(spark.range(5).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2. Using Docker to Set Up Spark\n",
    "\n",
    "An alternative way is to run [Apache Spark](#apache-spark) inside Docker containers. This avoids manual setup and ensures a clean environment.\n",
    "\n",
    "#### 9.2.1. Prerequisites\n",
    "- Docker installed on your system ([Install Docker](https://docs.docker.com/get-docker/))\n",
    "\n",
    "#### 9.2.2. Standalone Setup\n",
    "\n",
    "##### 9.2.2.1. Pull a Spark Docker Image\n",
    "You can use an existing image from Docker Hub or customize it using a Dockerfile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker pull bitnami/spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.2.2.2. Run a Spark Container\n",
    "Start a Spark standalone container:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run -it bitnami/spark pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2.3. Set Up Spark Cluster\n",
    "You can create a local Spark cluster with [`docker-compose.yaml`](./docker-compose.yaml).\n",
    "\n",
    "##### 9.2.3.1. Start the Cluster\n",
    "Run the following command to start the cluster:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.2.3.2. Access the Spark Web UI\n",
    "- Master: [http://localhost:8080](http://localhost:8080)\n",
    "- Worker: [http://localhost:8081](http://localhost:8081)\n",
    "\n",
    "##### 9.2.3.3. Submit Jobs\n",
    "You can submit jobs using the spark-submit tool or run a PySpark shell inside the container:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker exec -it spark-master pyspark --master spark://spark-master:7077"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.2.3.4. Setting Up Jupyter Notebook Container for Spark (Optional)\n",
    "\n",
    "Running a Jupyter Notebook container alongside your Spark services is a great way to interactively test Spark code using PySpark.\n",
    "\n",
    "- Uncomment the `jupyter` service block in the [`docker-compose.yaml`](./docker-compose.yaml) file.\n",
    "- Ensure the `notebooks` directory exists in the same location as your `docker-compose.yaml`:\n",
    "    ```bash\n",
    "    mkdir notebooks\n",
    "    ```\n",
    "  This directory will be mounted into the Jupyter container so that your notebooks are saved persistently.\n",
    "\n",
    "- To start the whole cluster (including Jupyter):\n",
    "    ```bash\n",
    "    docker-compose up -d\n",
    "    ```\n",
    "- To start only the Jupyter container (after cluster is running):\n",
    "    ```bash\n",
    "    docker-compose up -d jupyter\n",
    "    ```\n",
    "- You can now access the notebook UI at: [http://localhost:8888](http://localhost:8888)\n",
    "\n",
    "  Use the token shown in the terminal (when the Jupyter container starts) to log in.\n",
    "\n",
    "##### 9.2.3.5. Test Notebook Code\n",
    "\n",
    "In a new notebook, run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NotebookSpark\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.range(5).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
