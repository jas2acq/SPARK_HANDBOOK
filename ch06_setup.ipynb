{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1691619e",
   "metadata": {},
   "source": [
    "# Apache Spark Local Setup\n",
    "\n",
    "## Table of Contents\n",
    "- [1. Installing Spark Locally (Native Installation)](#1-installing-spark-locally-native-installation)\n",
    "  - [1.1 Prerequisites](#11-prerequisites)\n",
    "  - [1.2 Download and Install Spark](#12-download-and-install-spark)\n",
    "  - [1.3 Set Environment Variables](#13-set-environment-variables)\n",
    "  - [1.4 Install Required Python Libraries](#14-install-required-python-libraries)\n",
    "  - [1.5 Test Your Installation](#15-test-your-installation)\n",
    "- [2. Using Docker to Set Up Spark](#2-using-docker-to-set-up-spark)\n",
    "  - [2.1 Prerequisites](#21-prerequisites)\n",
    "  - [2.2 Standalone Setup](#22-standalone-setup)\n",
    "  - [2.3 Set Up Spark Cluster](#23-set-up-spark-cluster)\n",
    "    - [2.3.1 Start the Cluster](#231-start-the-cluster)\n",
    "    - [2.3.2 Access the Spark Web UI](#232-access-the-spark-web-ui)\n",
    "    - [2.3.3 Submit Jobs](#233-submit-jobs)\n",
    "    - [2.3.4 Setting Up Jupyter Notebook Container for Spark (Optional)](#234-setting-up-jupyter-notebook-container-for-spark-optional)\n",
    "    - [2.3.5 Test Notebook Code](#235-test-notebook-code)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81124ed8",
   "metadata": {},
   "source": [
    "\n",
    "In this section, we'll cover two common ways to set up Apache Spark on a local development machine:\n",
    "\n",
    "1. **Installing Spark Locally (Native Installation)**\n",
    "2. **Using Docker to Set Up Spark**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0bd3b4",
   "metadata": {},
   "source": [
    "## 1. Installing Spark Locally (Native Installation)\n",
    "\n",
    "This method involves manually installing Spark and its dependencies on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43acb95",
   "metadata": {},
   "source": [
    "### 1.1 Prerequisites\n",
    "- **Java (JDK 8 or 11):** Spark runs on the JVM.\n",
    "- **Python 3.x:** Required for PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd7cffd",
   "metadata": {},
   "source": [
    "### 1.2 Download and Install Spark\n",
    "- Download Spark from the [Official Apache Spark website](https://spark.apache.org/downloads.html).\n",
    "    - Choose a version (e.g., Spark 3.4.1) and a pre-built package for Hadoop (e.g., \"Pre-built for Apache Hadoop 3.3 and later\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff058e8",
   "metadata": {},
   "source": [
    "#### Extract the archive to a directory of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064dd8c0",
   "metadata": {},
   "source": [
    "**On Linux:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ddf088",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "tar -xzf spark-3.4.1-bin-hadoop3.tgz -C /path/to/your/directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd175192",
   "metadata": {},
   "source": [
    "**On Windows:**\n",
    "- Use a tool like 7-Zip or WinRAR.\n",
    "    - Right-click the downloaded `.tgz` file\n",
    "    - Select \"Extract Here\" or \"Extract to spark-3.4.1-bin-hadoop3\"\n",
    "    - Move the extracted folder to your desired location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2108a93",
   "metadata": {},
   "source": [
    "### 1.3 Set Environment Variables\n",
    "Set the following environment variables so your system can find Spark and Java."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47575173",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Linux (add to ~/.bashrc or ~/.zshrc)\n",
    "export SPARK_HOME=/path/to/your/directory/spark-3.4.1-bin-hadoop3\n",
    "export PATH=$PATH:$SPARK_HOME/bin\n",
    "export JAVA_HOME=/path/to/your/java"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6560dc4",
   "metadata": {},
   "source": [
    "On Windows, set environment variables via System Properties > Environment Variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55089f6",
   "metadata": {},
   "source": [
    "### 1.4 Install Required Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2beb33",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyspark findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a45c47",
   "metadata": {},
   "source": [
    "### 1.5 Test Your Installation\n",
    "Start the PySpark shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b2680a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce358b3",
   "metadata": {},
   "source": [
    "Or test with a small script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d759b66d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
    "print(spark.range(5).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06abb92",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Using Docker to Set Up Spark\n",
    "\n",
    "An alternative way is to run Spark inside Docker containers. This avoids manual setup and ensures a clean environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95d04bb",
   "metadata": {},
   "source": [
    "### 2.1 Prerequisites\n",
    "- Docker installed on your system ([Install Docker](https://docs.docker.com/get-docker/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45276219",
   "metadata": {},
   "source": [
    "### 2.2 Standalone Setup\n",
    "#### 2.2.1 Pull a Spark Docker image\n",
    "You can use an existing image from Docker Hub or customize it using a Dockerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b70c84",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker pull bitnami/spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0235dd8f",
   "metadata": {},
   "source": [
    "#### 2.2.2 Run a Spark Container\n",
    "Start a Spark standalone container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e39ea",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run -it bitnami/spark pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487eacbf",
   "metadata": {},
   "source": [
    "### 2.3 Set Up Spark Cluster\n",
    "You can create a local Spark cluster with [`docker-compose.yaml`](./docker-compose.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185e0001",
   "metadata": {},
   "source": [
    "#### 2.3.1 Start the Cluster\n",
    "Run the following command to start the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4577c11",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba9e1c8",
   "metadata": {},
   "source": [
    "#### 2.3.2 Access the Spark Web UI\n",
    "- Master: [http://localhost:8080](http://localhost:8080)\n",
    "- Worker: [http://localhost:8081](http://localhost:8081)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f65a9b1",
   "metadata": {},
   "source": [
    "#### 2.3.3 Submit Jobs\n",
    "You can submit jobs using the spark-submit tool or run a PySpark shell inside the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fbcdeb",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker exec -it spark-master pyspark --master spark://spark-master:7077"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7914a3",
   "metadata": {},
   "source": [
    "#### 2.3.4 Setting Up Jupyter Notebook Container for Spark (Optional)\n",
    "\n",
    "Running a Jupyter Notebook container alongside your Spark services is a great way to interactively test Spark code using PySpark.\n",
    "\n",
    "- Uncomment the `jupyter` service block in the [`docker-compose.yaml`](./docker-compose.yaml) file.\n",
    "- Ensure the `notebooks` directory exists in the same location as your `docker-compose.yaml`:\n",
    "    ```bash\n",
    "    mkdir notebooks\n",
    "    ```\n",
    "  This directory will be mounted into the Jupyter container so that your notebooks are saved persistently.\n",
    "\n",
    "- To start the whole cluster (including Jupyter):\n",
    "    ```bash\n",
    "    docker-compose up -d\n",
    "    ```\n",
    "- To start only the Jupyter container (after cluster is running):\n",
    "    ```bash\n",
    "    docker-compose up -d jupyter\n",
    "    ```\n",
    "- You can now access the notebook UI at: [http://localhost:8888](http://localhost:8888)\n",
    "\n",
    "  Use the token shown in the terminal (when the Jupyter container starts) to log in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74708543",
   "metadata": {},
   "source": [
    "#### 2.3.5 Test Notebook Code\n",
    "\n",
    "In a new notebook, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8445eec",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NotebookSpark\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.range(5).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
