{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c615a74",
   "metadata": {},
   "source": [
    "# Spark Handbook\n",
    "## Apache Spark: A Comprehensive Guide for Data Engineers\n",
    "This handbook provides a comprehensive overview of [Apache Spark](glossary.md#apache-spark), a powerful [distributed data processing framework](glossary.md#distributed-data-processing-framework) designed for handling [big data](glossary.md#big-data) workloads with speed, ease of use, and flexibility.\n",
    "\n",
    "## Table of Contents\n",
    "- [How Apache Spark Works](#how-apache-spark-works)\n",
    "- [Apache Spark Architecture](#apache-spark-architecture)\n",
    "- [Spark Core Components and Libraries](#spark-core-components-and-libraries)\n",
    "- [Why Do Data Engineers Need Spark?](#why-do-data-engineers-need-spark)\n",
    "- [Typical Use Cases](#typical-use-cases)\n",
    "- [RDDs and DataFrames in Apache Spark](#rdds-and-dataframes-in-apache-spark)\n",
    "  - [Introduction](#1-introduction)\n",
    "  - [RDD: Resilient Distributed Dataset](#2-rdd-resilient-distributed-dataset)\n",
    "  - [DataFrames](#3-dataframes)\n",
    "  - [Conversion Between RDD and DataFrame](#4-conversion-between-rdd-and-dataframe)\n",
    "  - [RDD vs. DataFrame - Comparison](#5-rdd-vs-dataframe---comparison)\n",
    "  - [Use Case Summary](#6-use-case-summary)\n",
    "  - [Conclusion](#7-conclusion)\n",
    "- [Apache Spark Local Setup](#apache-spark-local-setup)\n",
    "  - [Installing Spark Locally (Native Installation)](#1-installing-spark-locally-native-installation)\n",
    "  - [Using Docker to Set Up Spark](#2-using-docker-to-set-up-spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97879eb",
   "metadata": {},
   "source": [
    "## What is Apache Spark?\n",
    "\n",
    "**Apache Spark** is an open-source, distributed analytics engine designed for large-scale data processing and machine learning. It is renowned for its speed, versatility, and ability to scale from a single machine to large clusters of computers. Spark offers APIs in several popular languages, including Python (using PySpark), Scala, Java, and R, making it accessible to a wide audience of data professionals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dac616d",
   "metadata": {},
   "source": [
    "## Main Abstractions of Apache Spark\n",
    "\n",
    "### Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "RDDs are the fundamental data structure in Spark. They represent immutable, distributed collections of objects partitioned across the cluster. RDDs support two types of operations:\n",
    "\n",
    "- **Transformations:** Lazy operations that define a new RDD from an existing one (e.g., map, filter).\n",
    "- **Actions:** Operations that trigger computation and return results (e.g., collect, count).\n",
    "\n",
    "RDDs enable fault tolerance by tracking lineage, allowing the system to recompute lost data partitions in case of node failure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c67496b",
   "metadata": {},
   "source": [
    "### Directed Acyclic Graph (DAG)\n",
    "\n",
    "Spark uses a DAG to represent the sequence of transformations applied to RDDs. When a job is submitted, Spark’s DAG scheduler breaks the computation into stages of tasks that can be executed in parallel. This DAG-based execution plan enables optimization and efficient job scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7628327e",
   "metadata": {},
   "source": [
    "## Spark Architecture\n",
    "\n",
    "Apache Spark follows a **master-worker** architecture composed of several key components:\n",
    "\n",
    "### 1. Spark Driver\n",
    "\n",
    "The driver is the central coordinator of a Spark application. It:\n",
    "- Maintains the lifecycle of the application.\n",
    "- Converts user code into a logical execution plan (DAG).\n",
    "- Schedules tasks and monitors their execution.\n",
    "- Communicates with the cluster manager to request resources.\n",
    "- Collects and aggregates results from executors.\n",
    "\n",
    "The driver contains components such as SparkContext, DAG Scheduler, Task Scheduler, and Block Manager."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0995b8b",
   "metadata": {},
   "source": [
    "### 2. Cluster Manager\n",
    "\n",
    "The cluster manager oversees resource allocation in the cluster. It manages CPUs, memory, and executors across worker nodes. Spark can run on various cluster managers such as:\n",
    "- Apache YARN (Hadoop ecosystem)\n",
    "- Apache Mesos\n",
    "- Kubernetes\n",
    "- Spark's standalone cluster manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea4bb3",
   "metadata": {},
   "source": [
    "### 3. Executors\n",
    "\n",
    "Executors are worker processes launched on cluster nodes. Each executor:\n",
    "- Executes tasks assigned by the driver.\n",
    "- Performs computations on partitions of data.\n",
    "- Caches data in memory or on disk as needed.\n",
    "- Reports task status and results back to the driver.\n",
    "\n",
    "Executors live for the duration of a Spark application and enable parallel task execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bf6e8c",
   "metadata": {},
   "source": [
    "### 4. Worker Nodes\n",
    "\n",
    "Worker nodes are the physical or virtual machines in the cluster where executors run. They host one or more executors executing tasks in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b85d9f0",
   "metadata": {},
   "source": [
    "### 5. SparkContext\n",
    "\n",
    "SparkContext is the entry point through which the driver interacts with the cluster. It:\n",
    "- Connects to the cluster manager.\n",
    "- Creates RDDs and manages their lifecycle.\n",
    "- Coordinates job execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b8a2ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Spark Core Components and Libraries\n",
    "\n",
    "### Spark SQL\n",
    "\n",
    "Spark SQL is Spark’s module for working with structured data. It allows querying data using:\n",
    "- Standard SQL.\n",
    "- Hive Query Language (HQL).\n",
    "- Support for numerous data sources including Hive tables, Parquet, and JSON.\n",
    "\n",
    "Spark SQL integrates SQL queries with Spark’s programmatic APIs (RDDs, DataFrames) in Python, Scala, and Java. This tight integration supports complex analytics and interactive querying within a unified application framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a3dc74",
   "metadata": {},
   "source": [
    "### MLlib\n",
    "\n",
    "MLlib is Spark’s scalable machine learning library. It provides:\n",
    "- Algorithms for classification, regression, clustering, and collaborative filtering.\n",
    "- Utilities for model evaluation and data import.\n",
    "- Low-level primitives such as a generic gradient descent optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ae6a7e",
   "metadata": {},
   "source": [
    "### GraphX\n",
    "\n",
    "GraphX is Spark’s graph processing library, enabling:\n",
    "- Creation and manipulation of graphs with properties on vertices and edges.\n",
    "- Graph-parallel computations like PageRank and triangle counting.\n",
    "- Operators such as subgraph extraction and vertex mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c0826",
   "metadata": {},
   "source": [
    "## Why Do Data Engineers Need Spark?\n",
    "\n",
    "### 1. Speed and Performance\n",
    "- Spark performs in-memory computing, reducing costly disk read/write operations.\n",
    "- It can be up to 100× faster than Hadoop MapReduce for iterative and interactive workloads.\n",
    "\n",
    "### 2. Scalability\n",
    "- Spark scales from a single machine to thousands of cluster nodes.\n",
    "- Handles petabyte-scale data through distributed processing.\n",
    "\n",
    "### 3. Unified Processing Engine\n",
    "- Supports batch processing, real-time streaming, SQL querying, machine learning, and graph analytics all within one platform.\n",
    "\n",
    "### 4. Language Flexibility and Ease of Use\n",
    "- Provides APIs in Python, Scala, Java, and R.\n",
    "- High-level abstractions (RDDs, DataFrames, Datasets) simplify complex data transformations.\n",
    "\n",
    "### 5. Ecosystem and Integration\n",
    "- Integrates with Hadoop HDFS, Amazon S3, Apache Kafka, and other platforms.\n",
    "- Supports multiple cluster managers for flexible deployment.\n",
    "\n",
    "### 6. Essential for Modern Workloads\n",
    "- Enables ETL pipelines, real-time dashboards, machine learning workflows, and large-scale interactive queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4bbb3f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Typical Use Cases\n",
    "- ETL pipelines for big data ingestion and transformation\n",
    "- Scalable machine learning model training and deployment\n",
    "- Real-time data stream processing (e.g., fraud detection, log analysis)\n",
    "- Graph analytics for social network analysis and recommendations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2349f9c1",
   "metadata": {},
   "source": [
    "# RDDs and DataFrames in Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e0a8fd",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Apache Spark has two core abstractions for working with distributed data:\n",
    "- **RDD (Resilient Distributed Dataset):** The original low-level distributed data structure\n",
    "- **DataFrame:** A high-level abstraction built on top of RDDs, offering a tabular data structure similar to a database table or Pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb91f818",
   "metadata": {},
   "source": [
    "## 2. RDD: Resilient Distributed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ca31a",
   "metadata": {},
   "source": [
    "### 2.1 What is an RDD?\n",
    "An RDD is an immutable distributed collection of objects that can be processed in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436dbb0d",
   "metadata": {},
   "source": [
    "### 2.2 Key Features\n",
    "- Fault-tolerant\n",
    "- Lazy evaluation\n",
    "- Supports transformations (`map`, `filter`, etc.) and actions (`collect`, `count`, etc.)\n",
    "- Type-safe (in Scala/Java)\n",
    "- No built-in schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d32c14",
   "metadata": {},
   "source": [
    "### 2.3 Creating or Loading Data into an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e083a0",
   "metadata": {},
   "source": [
    "#### Creating an RDD (PySpark):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d007ef",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RDDExample\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27779b4",
   "metadata": {},
   "source": [
    "#### Loading Data into an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d5532",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Load file (skip header)\n",
    "rdd = sc.textFile(\"./data/customers.csv\")\n",
    "header = rdd.first()\n",
    "rdd_data = rdd.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f754c6",
   "metadata": {},
   "source": [
    "### 2.4 RDD Transformation and Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe76def",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Split CSV into fields\n",
    "customers_rdd = rdd_data.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc83956",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# View Sample\n",
    "customers_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db71fa6",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Count missing join dates\n",
    "missing_dates = customers_rdd.filter(lambda x: x[4] == \"\").count()\n",
    "print(f\"Missing join dates: {missing_dates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aff271",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Extract customer names\n",
    "names = customers_rdd.map(lambda x: f\"{x[1]} {x[2]}\").collect()\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dc97ba",
   "metadata": {},
   "source": [
    "## 3. DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cadf99",
   "metadata": {},
   "source": [
    "### 3.1 What is a DataFrame?\n",
    "A DataFrame is a distributed collection of data organized into named columns, like a SQL table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfed20cd",
   "metadata": {},
   "source": [
    "### 3.2 Key Features\n",
    "- Schema-aware (columns and types)\n",
    "- Optimized by Catalyst optimizer\n",
    "- Supports SQL queries via `spark.sql()`\n",
    "- Interoperable with RDDs and Pandas\n",
    "- Better performance than RDD for most use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3fc475",
   "metadata": {},
   "source": [
    "### 3.3 Creating or Loading Data into a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdca95d3",
   "metadata": {},
   "source": [
    "#### Reading CSV into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3437db6",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", True).csv(\"./data/customers.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecec80e",
   "metadata": {},
   "source": [
    "### 3.4 Common DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad07543",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Print schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75751ac9",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "df.select(\"first_name\", \"email\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce9452",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Filter customers with missing join dates\n",
    "df.filter(df.join_date.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e98f4",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Count customers who joined\n",
    "df.filter(df.join_date.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4dad45",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Extract customer names (from RDD)\n",
    "names = customers_rdd.map(lambda x: f\"{x[1]} {x[2]}\").collect()\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1864bd1d",
   "metadata": {},
   "source": [
    "## 4. Conversion Between RDD and DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290daf6f",
   "metadata": {},
   "source": [
    "### From RDD to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea8354a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Convert RDD to Row RDD\n",
    "row_rdd = customers_rdd.map(lambda x: Row(\n",
    "    customer_id=int(x[0]),\n",
    "    first_name=x[1],\n",
    "    last_name=x[2],\n",
    "    email=x[3],\n",
    "    join_date=x[4] if x[4] != \"\" else None\n",
    "))\n",
    "\n",
    "df_from_rdd = spark.createDataFrame(row_rdd)\n",
    "df_from_rdd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a1415",
   "metadata": {},
   "source": [
    "### From DataFrame to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6991d0c7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "rdd_from_df = df.rdd\n",
    "rdd_from_df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e504791b",
   "metadata": {},
   "source": [
    "## 5. RDD vs. DataFrame - Comparison\n",
    "\n",
    "| Feature           | RDD                        | DataFrame               |\n",
    "| ----------------- | -------------------------- | ----------------------- |\n",
    "| Abstraction Level | Low                        | High                    |\n",
    "| API Style         | Functional                 | SQL-like                |\n",
    "| Schema            | Not enforced               | Schema-aware            |\n",
    "| Performance       | Lower                      | Optimized with Catalyst |\n",
    "| Best for          | Custom, fine-grained logic | Queries, aggregations   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7311addd",
   "metadata": {},
   "source": [
    "## 6. Use Case Summary\n",
    "\n",
    "| Task                                     | Recommended |\n",
    "| ---------------------------------------- | ----------- |\n",
    "| Load structured CSV data                 | DataFrame   |\n",
    "| Filter or select fields efficiently      | DataFrame   |\n",
    "| Custom parsing, transformation, or logic | RDD         |\n",
    "| SQL-like querying and grouping           | DataFrame   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aa747f",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "- Use DataFrames when working with structured data like CSV, JSON, or Parquet.\n",
    "- Use RDDs when you need custom logic, performance tuning, or low-level transformations.\n",
    "\n",
    "This practical section using your `customers.csv` helps you clearly see how both abstractions work and when to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edba985",
   "metadata": {},
   "source": [
    "# Apache Spark Local Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bfec0c",
   "metadata": {},
   "source": [
    "In this section, we'll cover two common ways to set up Apache Spark on a local development machine:\n",
    "\n",
    "1. **Installing Spark Locally (Native Installation)**\n",
    "2. **Using Docker to Set Up Spark**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72da0ab",
   "metadata": {},
   "source": [
    "## 1. Installing Spark Locally (Native Installation)\n",
    "\n",
    "This method involves manually installing Spark and its dependencies on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f341eaa",
   "metadata": {},
   "source": [
    "### 1.1 Prerequisites\n",
    "- **Java (JDK 8 or 11):** Spark runs on the JVM.\n",
    "- **Python 3.x:** Required for PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e772461",
   "metadata": {},
   "source": [
    "### 1.2 Download and Install Spark\n",
    "- Download Spark from the [Official Apache Spark website](https://spark.apache.org/downloads.html).\n",
    "    - Choose a version (e.g., Spark 3.4.1) and a pre-built package for Hadoop (e.g., \"Pre-built for Apache Hadoop 3.3 and later\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ada4129",
   "metadata": {},
   "source": [
    "#### Extract the archive to a directory of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb00c24",
   "metadata": {},
   "source": [
    "**On Linux:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf77d8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "tar -xzf spark-3.4.1-bin-hadoop3.tgz -C /path/to/your/directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682232d9",
   "metadata": {},
   "source": [
    "**On Windows:**\n",
    "- Use a tool like 7-Zip or WinRAR.\n",
    "    - Right-click the downloaded `.tgz` file\n",
    "    - Select \"Extract Here\" or \"Extract to spark-3.4.1-bin-hadoop3\"\n",
    "    - Move the extracted folder to your desired location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72f571e",
   "metadata": {},
   "source": [
    "### 1.3 Set Environment Variables\n",
    "Set the following environment variables so your system can find Spark and Java."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3817875d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Linux (add to ~/.bashrc or ~/.zshrc)\n",
    "export SPARK_HOME=/path/to/your/directory/spark-3.4.1-bin-hadoop3\n",
    "export PATH=$PATH:$SPARK_HOME/bin\n",
    "export JAVA_HOME=/path/to/your/java"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc798ab",
   "metadata": {},
   "source": [
    "On Windows, set environment variables via System Properties > Environment Variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861d7b5b",
   "metadata": {},
   "source": [
    "### 1.4 Install Required Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc820efe",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyspark findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7f9047",
   "metadata": {},
   "source": [
    "### 1.5 Test Your Installation\n",
    "Start the PySpark shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36864dfd",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01858e",
   "metadata": {},
   "source": [
    "Or test with a small script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfcb1b1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
    "print(spark.range(5).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4f58a2",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Using Docker to Set Up Spark\n",
    "\n",
    "An alternative way is to run Spark inside Docker containers. This avoids manual setup and ensures a clean environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56a1f10",
   "metadata": {},
   "source": [
    "### 2.1 Prerequisites\n",
    "- Docker installed on your system ([Install Docker](https://docs.docker.com/get-docker/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595655e4",
   "metadata": {},
   "source": [
    "### 2.2 Standalone Setup\n",
    "#### 2.2.1 Pull a Spark Docker image\n",
    "You can use an existing image from Docker Hub or customize it using a Dockerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b834f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker pull bitnami/spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee2a214",
   "metadata": {},
   "source": [
    "#### 2.2.2 Run a Spark Container\n",
    "Start a Spark standalone container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c4128d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run -it bitnami/spark pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d1204d",
   "metadata": {},
   "source": [
    "### 2.3 Set Up Spark Cluster\n",
    "You can create a local Spark cluster with [`docker-compose.yaml`](./docker-compose.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7577eceb",
   "metadata": {},
   "source": [
    "#### 2.3.1 Start the Cluster\n",
    "Run the following command to start the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1b31f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820a333",
   "metadata": {},
   "source": [
    "#### 2.3.2 Access the Spark Web UI\n",
    "- Master: [http://localhost:8080](http://localhost:8080)\n",
    "- Worker: [http://localhost:8081](http://localhost:8081)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88072a97",
   "metadata": {},
   "source": [
    "#### 2.3.3 Submit Jobs\n",
    "You can submit jobs using the spark-submit tool or run a PySpark shell inside the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af7797",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker exec -it spark-master pyspark --master spark://spark-master:7077"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd69293",
   "metadata": {},
   "source": [
    "#### 2.3.4 Setting Up Jupyter Notebook Container for Spark (Optional)\n",
    "\n",
    "Running a Jupyter Notebook container alongside your Spark services is a great way to interactively test Spark code using PySpark.\n",
    "\n",
    "- Uncomment the `jupyter` service block in the [`docker-compose.yaml`](./docker-compose.yaml) file.\n",
    "- Ensure the `notebooks` directory exists in the same location as your `docker-compose.yaml`:\n",
    "    ```bash\n",
    "    mkdir notebooks\n",
    "    ```\n",
    "  This directory will be mounted into the Jupyter container so that your notebooks are saved persistently.\n",
    "\n",
    "- To start the whole cluster (including Jupyter):\n",
    "    ```bash\n",
    "    docker-compose up -d\n",
    "    ```\n",
    "- To start only the Jupyter container (after cluster is running):\n",
    "    ```bash\n",
    "    docker-compose up -d jupyter\n",
    "    ```\n",
    "- You can now access the notebook UI at: [http://localhost:8888](http://localhost:8888)\n",
    "\n",
    "  Use the token shown in the terminal (when the Jupyter container starts) to log in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a89c5",
   "metadata": {},
   "source": [
    "#### 2.3.5 Test Notebook Code\n",
    "\n",
    "In a new notebook, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e990c0d9",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NotebookSpark\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.range(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb69676",
   "metadata": {},
   "source": [
    "# Extracting and Transforming Data with Apache Spark\n",
    "\n",
    "This section provides a comprehensive guide to extracting and transforming data using Apache Spark, focusing on Spark SQL and DataFrame APIs. It includes detailed explanations, practical scenarios, code examples, and best practices to help you master these critical aspects of Spark.\n",
    "\n",
    "### Extracting Data with Spark\n",
    "\n",
    "#### Overview\n",
    "Extracting data in Apache Spark involves loading data from various sources into DataFrames for further processing. Spark’s DataFrame API provides a unified interface to read data from file formats like JSON, CSV, and Parquet, as well as databases and cloud storage.\n",
    "\n",
    "#### Supported Data Sources\n",
    "Spark supports a wide range of data sources, including:\n",
    "- `File Formats`: CSV, JSON, Parquet, ORC, Avro, Text\n",
    "- `Databases`: JDBC/ODBC (MySQL, PostgreSQL, SQL Server, etc.)\n",
    "- `Big Data Systems`: Hadoop HDFS, Apache Hive, Apache HBase\n",
    "- `Cloud Storage`: AWS S3, Google Cloud Storage, Azure Blob Storage\n",
    "- `Other`: Kafka, NoSQL databases like Cassandra\n",
    "\n",
    "#### Reading Data\n",
    "Spark provides the `spark.read` API to load data into DataFrames. Common methods include:\n",
    "- `spark.read.csv(path)`: Reads CSV files\n",
    "- `spark.read.json(path)`: Reads JSON files\n",
    "- `spark.read.parquet(path)`: Reads Parquet files.\n",
    "- ``spark.read.jdbc(url, table, properties)``: Reads from JDBC databases.\n",
    "\n",
    "#### Key Options:\n",
    "- `header=True`: Treats the first row as column names (CSV).\n",
    "- `inferSchema=True`: Automatically infers column data types.\n",
    "- `schema=StructType`: Specifies a custom schema to avoid inference overhead.\n",
    "- `mode`: Controls error handling (permissive, dropmalformed, failfast).\n",
    "\n",
    "#### Scenario: Extracting Data from JSON, CSV, and Parquet\n",
    "**Problem**: Load the `products.json`, `customers.csv`, and `orders.parquet` datasets into Spark DataFrames, ensuring proper schema handling and error management for missing or inconsistent data.\n",
    "\n",
    "**Solution**: <br>\n",
    "- Define explicit schemas to ensure correct data types.\n",
    "- Handle missing or malformed data during extraction.\n",
    "- Cache DataFrames for repeated use and save them in Parquet for unified storage.\n",
    "\n",
    "**Code Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70081370",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products:\n",
      "+----------+------------+-----------+------+\n",
      "|product_id|product_name|category   |price |\n",
      "+----------+------------+-----------+------+\n",
      "|101       |Laptop      |Electronics|1200.5|\n",
      "|102       |Mouse       |Accessories|25.0  |\n",
      "|103       |Keyboard    |Accessories|75.75 |\n",
      "|104       |Monitor     |Electronics|300.0 |\n",
      "|105       |USB Cable   |accessories|10.0  |\n",
      "+----------+------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "Customers:\n",
      "+-----------+----------+---------+-----------------------+----------+\n",
      "|customer_id|first_name|last_name|email                  |join_date |\n",
      "+-----------+----------+---------+-----------------------+----------+\n",
      "|1          |John      |Doe      |john.doe@example.com   |2023-01-15|\n",
      "|2          |Jane      |Smith    |jane.smith@example.com |2023-02-20|\n",
      "|3          |Peter     |Jones    |peter.jones@example.com|2023-03-10|\n",
      "|4          |Sarah     |Lee      |sarah.lee@example.com  |NULL      |\n",
      "|5          |Mike      |NULL     |mike.brown@example.com |2023-05-01|\n",
      "+-----------+----------+---------+-----------------------+----------+\n",
      "only showing top 5 rows\n",
      "Orders:\n",
      "+--------+-----------+----------+--------+-----------+\n",
      "|order_id|customer_id|product_id|quantity|total_price|\n",
      "+--------+-----------+----------+--------+-----------+\n",
      "|1001    |1          |101       |1       |1200.5     |\n",
      "|1002    |2          |102       |2       |50.0       |\n",
      "|1003    |3          |103       |1       |75.75      |\n",
      "|1004    |1          |104       |1       |300.0      |\n",
      "|1005    |5          |105       |5       |50.0       |\n",
      "+--------+-----------+----------+--------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkHandbook\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for products.json\n",
    "products_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), False),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for customers.csv\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"join_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Read products.json\n",
    "products_df = spark.read \\\n",
    "    .schema(products_schema) \\\n",
    "    .json(\"./data/products.json\")\n",
    " \n",
    "\n",
    "# Read customers.csv\n",
    "customers_df = spark.read \\\n",
    "    .schema(customers_schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"dropmalformed\") \\\n",
    "    .csv(\"./data/customers.csv\")\n",
    "\n",
    "# Read orders.parquet\n",
    "orders_df = spark.read.parquet(\"./data/orders_parquet/orders.parquet\")\n",
    "\n",
    "# Cache DataFrames for performance\n",
    "products_df.cache()\n",
    "customers_df.cache()\n",
    "orders_df.cache()\n",
    "\n",
    "# Show sample data\n",
    "print(\"Products:\")\n",
    "products_df.show(5, truncate=False)\n",
    "print(\"Customers:\")\n",
    "customers_df.show(5, truncate=False)\n",
    "print(\"Orders:\")\n",
    "orders_df.show(5, truncate=False)\n",
    "\n",
    "# Save to Parquet for unified storage\n",
    "products_df.write.mode(\"overwrite\").parquet(\"./data/cleaned/products_clean.parquet\")\n",
    "customers_df.write.mode(\"overwrite\").parquet(\"./data/cleaned/customers_clean.parquet\")\n",
    "orders_df.write.mode(\"overwrite\").parquet(\"./data/cleaned/orders_clean.parquet\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c5180",
   "metadata": {},
   "source": [
    "**Explanation**:\n",
    "- Defines explicit schemas to ensure correct data types and avoid inference overhead.\n",
    "- Uses `dropmalformed` mode for the CSV to skip any malformed rows.\n",
    "- Reads `orders.parquet` directly, as Parquet files include schema metadata.\n",
    "- Caches DataFrames to improve performance for subsequent transformations.\n",
    "- Saves cleaned DataFrames to Parquet for efficient storage and querying."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba00f1",
   "metadata": {},
   "source": [
    "### Transforming Data with Spark\n",
    "**Overview** <br>\n",
    "Transforming data in Spark involves manipulating DataFrames to clean, enrich, or aggregate data for analysis. Spark’s DataFrame API and SQL queries support operations like joins, aggregations, filtering, and window functions, optimized for distributed execution. The provided datasets will be used to demonstrate these transformations in practical scenarios.\n",
    "\n",
    "### Common Transformation Operations\n",
    "**Joins**: <br>\n",
    "Joins combine DataFrames based on a key. Common types include:\n",
    "- `Inner Join`: Returns only matching rows.\n",
    "- `Left Outer Join`: Includes all rows from the left DataFrame, with nulls for non-matching rows.\n",
    "- `Right Outer Join`: Includes all rows from the right DataFrame.\n",
    "- `Full Outer Join`: Includes all rows from both DataFrames.\n",
    "\n",
    "**Syntax**:\n",
    "```python \n",
    "result_df = df1.join(df2, df1.key == df2.key, \"inner\")\n",
    "```\n",
    "\n",
    "**Aggegations**: <br>\n",
    "Aggregations summarize data using functions like `sum`, `avg`, `count`, `min`, `max`, typically after `groupBy`\n",
    "\n",
    "**Syntax**: <br>\n",
    "```python \n",
    "agg_df = df.groupBy(\"column\").agg({\"other_column\": \"sum\"})\n",
    "```\n",
    "\n",
    "**Filtering and Selecting**: <br>\n",
    "- **Filtering**: Select rows with filter or where.\n",
    "- **Selecting**: Choose columns with select.\n",
    "\n",
    "**Syntax**:\n",
    "```python \n",
    "filtered_df = df.filter(col(\"column\") > value)\n",
    "selected_df = df.select(\"column1\", \"column2\")\n",
    "```\n",
    "\n",
    "**Handling Missing Data**\n",
    "- `Drop nulls`: df.dropna(subset=[\"column\"])\n",
    "- `Fill nulls`: df.fillna(value, subset=[\"column\"])\n",
    "- `Replace values`: df.replace(old_value, new_value, subset=[\"column\"])\n",
    "\n",
    "\n",
    "### Scenario: Joining and Aggregating Sales Data\n",
    "**Problem**:\n",
    "Combine the `products`, `customers`, and `orders` datasets to calculate total sales and average order amount per product category. Standardize the category column in products and handle missing data in customers.\n",
    "\n",
    "**Solution**: <br>\n",
    "- Join the three DataFrames on appropriate keys.\n",
    "- Standardize `category` in `products`.\n",
    "- Handle missing `last_name` and `join_date` in customers.\n",
    "- Aggregate by `category` to compute sales metrics.\n",
    "\n",
    "#### Code Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd5404b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined DataFrame:\n",
      "+----------+-----------+--------+--------+-----------+----------+---------+-----------------------+----------+------------+-----------+------+\n",
      "|product_id|customer_id|order_id|quantity|total_price|first_name|last_name|email                  |join_date |product_name|category   |price |\n",
      "+----------+-----------+--------+--------+-----------+----------+---------+-----------------------+----------+------------+-----------+------+\n",
      "|101       |1          |1001    |1       |1200.5     |John      |Doe      |john.doe@example.com   |2023-01-15|Laptop      |ELECTRONICS|1200.5|\n",
      "|102       |2          |1002    |2       |50.0       |Jane      |Smith    |jane.smith@example.com |2023-02-20|Mouse       |ACCESSORIES|25.0  |\n",
      "|103       |3          |1003    |1       |75.75      |Peter     |Jones    |peter.jones@example.com|2023-03-10|Keyboard    |ACCESSORIES|75.75 |\n",
      "|104       |1          |1004    |1       |300.0      |John      |Doe      |john.doe@example.com   |2023-01-15|Monitor     |ELECTRONICS|300.0 |\n",
      "|105       |5          |1005    |5       |50.0       |Mike      |Unknown  |mike.brown@example.com |2023-05-01|USB Cable   |ACCESSORIES|10.0  |\n",
      "|106       |2          |1006    |1       |95.5       |Jane      |Smith    |jane.smith@example.com |2023-02-20|Webcam      |ELECTRONICS|95.5  |\n",
      "|101       |4          |1007    |2       |2401.0     |Sarah     |Lee      |sarah.lee@example.com  |2023-01-01|Laptop      |ELECTRONICS|1200.5|\n",
      "|107       |6          |1008    |1       |150.0      |Anna      |Garcia   |anna.garcia@example.com|2023-06-12|Headphones  |AUDIO      |150.0 |\n",
      "|102       |8          |1009    |3       |75.0       |Lisa      |Chen     |lisa.chen@example.com  |2023-08-01|Mouse       |ACCESSORIES|25.0  |\n",
      "+----------+-----------+--------+--------+-----------+----------+---------+-----------------------+----------+------------+-----------+------+\n",
      "\n",
      "Sales Summary:\n",
      "+-----------+-----------+----------------+-----------+\n",
      "|category   |total_sales|avg_order_amount|order_count|\n",
      "+-----------+-----------+----------------+-----------+\n",
      "|ACCESSORIES|250.75     |62.69           |4          |\n",
      "|ELECTRONICS|3997.00    |999.25          |4          |\n",
      "|AUDIO      |150.00     |150.00          |1          |\n",
      "+-----------+-----------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as sum_, avg, count, coalesce, lit, upper\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SparkTransformation\").getOrCreate()\n",
    "\n",
    "# Load data\n",
    "products_df = spark.read.parquet(\"./data/cleaned/products_clean.parquet\")\n",
    "customers_df = spark.read.parquet(\"./data/cleaned/customers_clean.parquet\")\n",
    "orders_df = spark.read.parquet(\"./data/cleaned/orders_clean.parquet\")\n",
    "\n",
    "# Standardize category in products (convert to uppercase)\n",
    "products_df = products_df.withColumn(\"category\", upper(col(\"category\")))\n",
    "\n",
    "# Handle missing data in customers\n",
    "customers_df = customers_df \\\n",
    "    .withColumn(\"last_name\", coalesce(col(\"last_name\"), lit(\"Unknown\"))) \\\n",
    "    .withColumn(\"join_date\", coalesce(col(\"join_date\"), lit(\"2023-01-01\").cast(DateType())))\n",
    "\n",
    "# Join DataFrames\n",
    "joined_df = orders_df \\\n",
    "    .join(customers_df, \"customer_id\", \"left_outer\") \\\n",
    "    .join(products_df, \"product_id\", \"inner\")\n",
    "\n",
    "# Print joined dataframe\n",
    "print(\"Joined DataFrame:\")\n",
    "joined_df.show(truncate=False)\n",
    "\n",
    "# Aggregate by category\n",
    "summary_df = joined_df.groupBy(\"category\") \\\n",
    "    .agg(\n",
    "        sum_(\"total_price\").alias(\"total_sales\"),\n",
    "        avg(\"total_price\").alias(\"avg_order_amount\"),\n",
    "        count(\"order_id\").alias(\"order_count\")\n",
    "    )\n",
    "\n",
    "# Format numerical columns\n",
    "summary_df = summary_df.select(\n",
    "    col(\"category\"),\n",
    "    col(\"total_sales\").cast(\"decimal(10,2)\"),\n",
    "    col(\"avg_order_amount\").cast(\"decimal(10,2)\"),\n",
    "    col(\"order_count\")\n",
    ")\n",
    "\n",
    "# Save and show results\n",
    "summary_df.write.mode(\"overwrite\").parquet(\"./data/cleaned/sales_summary.parquet\")\n",
    "print(\"Sales Summary:\")\n",
    "summary_df.show(truncate=False)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd832d5",
   "metadata": {},
   "source": [
    "**Explanation**: <br>\n",
    "- Standardizes `category` in `products_df` to uppercase to fix any inconsistencies (e.g., “accessories” vs. “Accessories”).\n",
    "- Fills missing `last_name` with “Unknown” and `join_date` with a default date in `customers_df`.\n",
    "- Performs a left outer join for `orders` and `customers` to include all orders, and an inner join with `products` to ensure valid products.\n",
    "- Aggregates by `category`, computing total sales, average order amount, and order count.\n",
    "- Formats numerical columns to `decimal(10,2)` for readability.\n",
    "- Saves results to Parquet.\n",
    "\n",
    "\n",
    "## Best Practices\n",
    "- Explicit Schemas: Always define schemas for JSON and CSV to ensure correct data types and avoid inference costs.\n",
    "- Handle Inconsistencies: Standardize case-sensitive fields (e.g., `category`) early in the pipeline.\n",
    "- Null Handling: Address missing data before joins or aggregations to prevent unexpected results.\n",
    "- Join Optimization: Use inner joins when possible; use left outer joins to preserve data when needed.\n",
    "- Columnar Storage: Use Parquet for intermediate and output data to leverage compression and columnar access.\n",
    "- SQL for Readability: Use Spark SQL for complex transformations when it improves clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fe9edd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
