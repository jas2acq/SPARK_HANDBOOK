{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c615a74",
   "metadata": {},
   "source": [
    "# Spark Handbook\n",
    "## Apache Spark: A Comprehensive Guide for Data Engineers\n",
    "This handbook provides a comprehensive overview of [Apache Spark](glossary.md#apache-spark), a powerful [distributed data processing framework](glossary.md#distributed-data-processing-framework) designed for handling [big data](glossary.md#big-data) workloads with speed, ease of use, and flexibility.\n",
    "\n",
    "## Table of Contents\n",
    "- [How Apache Spark Works](#how-apache-spark-works)\n",
    "- [Apache Spark Architecture](#apache-spark-architecture)\n",
    "- [Spark Core Components and Libraries](#spark-core-components-and-libraries)\n",
    "- [Why Do Data Engineers Need Spark?](#why-do-data-engineers-need-spark)\n",
    "- [Typical Use Cases](#typical-use-cases)\n",
    "- [RDDs and DataFrames in Apache Spark](#rdds-and-dataframes-in-apache-spark)\n",
    "  - [Introduction](#1-introduction)\n",
    "  - [RDD: Resilient Distributed Dataset](#2-rdd-resilient-distributed-dataset)\n",
    "  - [DataFrames](#3-dataframes)\n",
    "  - [Conversion Between RDD and DataFrame](#4-conversion-between-rdd-and-dataframe)\n",
    "  - [RDD vs. DataFrame - Comparison](#5-rdd-vs-dataframe---comparison)\n",
    "  - [Use Case Summary](#6-use-case-summary)\n",
    "  - [Conclusion](#7-conclusion)\n",
    "- [Apache Spark Local Setup](#apache-spark-local-setup)\n",
    "  - [Installing Spark Locally (Native Installation)](#1-installing-spark-locally-native-installation)\n",
    "  - [Using Docker to Set Up Spark](#2-using-docker-to-set-up-spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97879eb",
   "metadata": {},
   "source": [
    "## What is Apache Spark?\n",
    "\n",
    "**Apache Spark** is an open-source, distributed analytics engine designed for large-scale data processing and machine learning. It is renowned for its speed, versatility, and ability to scale from a single machine to large clusters of computers. Spark offers APIs in several popular languages, including Python (using PySpark), Scala, Java, and R, making it accessible to a wide audience of data professionals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dac616d",
   "metadata": {},
   "source": [
    "## Main Abstractions of Apache Spark\n",
    "\n",
    "### Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "RDDs are the fundamental data structure in Spark. They represent immutable, distributed collections of objects partitioned across the cluster. RDDs support two types of operations:\n",
    "\n",
    "- **Transformations:** Lazy operations that define a new RDD from an existing one (e.g., map, filter).\n",
    "- **Actions:** Operations that trigger computation and return results (e.g., collect, count).\n",
    "\n",
    "RDDs enable fault tolerance by tracking lineage, allowing the system to recompute lost data partitions in case of node failure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c67496b",
   "metadata": {},
   "source": [
    "### Directed Acyclic Graph (DAG)\n",
    "\n",
    "Spark uses a DAG to represent the sequence of transformations applied to RDDs. When a job is submitted, Spark’s DAG scheduler breaks the computation into stages of tasks that can be executed in parallel. This DAG-based execution plan enables optimization and efficient job scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7628327e",
   "metadata": {},
   "source": [
    "## Spark Architecture\n",
    "\n",
    "Apache Spark follows a **master-worker** architecture composed of several key components:\n",
    "\n",
    "### 1. Spark Driver\n",
    "\n",
    "The driver is the central coordinator of a Spark application. It:\n",
    "- Maintains the lifecycle of the application.\n",
    "- Converts user code into a logical execution plan (DAG).\n",
    "- Schedules tasks and monitors their execution.\n",
    "- Communicates with the cluster manager to request resources.\n",
    "- Collects and aggregates results from executors.\n",
    "\n",
    "The driver contains components such as SparkContext, DAG Scheduler, Task Scheduler, and Block Manager."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0995b8b",
   "metadata": {},
   "source": [
    "### 2. Cluster Manager\n",
    "\n",
    "The cluster manager oversees resource allocation in the cluster. It manages CPUs, memory, and executors across worker nodes. Spark can run on various cluster managers such as:\n",
    "- Apache YARN (Hadoop ecosystem)\n",
    "- Apache Mesos\n",
    "- Kubernetes\n",
    "- Spark's standalone cluster manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ea4bb3",
   "metadata": {},
   "source": [
    "### 3. Executors\n",
    "\n",
    "Executors are worker processes launched on cluster nodes. Each executor:\n",
    "- Executes tasks assigned by the driver.\n",
    "- Performs computations on partitions of data.\n",
    "- Caches data in memory or on disk as needed.\n",
    "- Reports task status and results back to the driver.\n",
    "\n",
    "Executors live for the duration of a Spark application and enable parallel task execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bf6e8c",
   "metadata": {},
   "source": [
    "### 4. Worker Nodes\n",
    "\n",
    "Worker nodes are the physical or virtual machines in the cluster where executors run. They host one or more executors executing tasks in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b85d9f0",
   "metadata": {},
   "source": [
    "### 5. SparkContext\n",
    "\n",
    "SparkContext is the entry point through which the driver interacts with the cluster. It:\n",
    "- Connects to the cluster manager.\n",
    "- Creates RDDs and manages their lifecycle.\n",
    "- Coordinates job execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b8a2ad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Spark Core Components and Libraries\n",
    "\n",
    "### Spark SQL\n",
    "\n",
    "Spark SQL is Spark’s module for working with structured data. It allows querying data using:\n",
    "- Standard SQL.\n",
    "- Hive Query Language (HQL).\n",
    "- Support for numerous data sources including Hive tables, Parquet, and JSON.\n",
    "\n",
    "Spark SQL integrates SQL queries with Spark’s programmatic APIs (RDDs, DataFrames) in Python, Scala, and Java. This tight integration supports complex analytics and interactive querying within a unified application framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a3dc74",
   "metadata": {},
   "source": [
    "### MLlib\n",
    "\n",
    "MLlib is Spark’s scalable machine learning library. It provides:\n",
    "- Algorithms for classification, regression, clustering, and collaborative filtering.\n",
    "- Utilities for model evaluation and data import.\n",
    "- Low-level primitives such as a generic gradient descent optimization algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ae6a7e",
   "metadata": {},
   "source": [
    "### GraphX\n",
    "\n",
    "GraphX is Spark’s graph processing library, enabling:\n",
    "- Creation and manipulation of graphs with properties on vertices and edges.\n",
    "- Graph-parallel computations like PageRank and triangle counting.\n",
    "- Operators such as subgraph extraction and vertex mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c0826",
   "metadata": {},
   "source": [
    "## Why Do Data Engineers Need Spark?\n",
    "\n",
    "### 1. Speed and Performance\n",
    "- Spark performs in-memory computing, reducing costly disk read/write operations.\n",
    "- It can be up to 100× faster than Hadoop MapReduce for iterative and interactive workloads.\n",
    "\n",
    "### 2. Scalability\n",
    "- Spark scales from a single machine to thousands of cluster nodes.\n",
    "- Handles petabyte-scale data through distributed processing.\n",
    "\n",
    "### 3. Unified Processing Engine\n",
    "- Supports batch processing, real-time streaming, SQL querying, machine learning, and graph analytics all within one platform.\n",
    "\n",
    "### 4. Language Flexibility and Ease of Use\n",
    "- Provides APIs in Python, Scala, Java, and R.\n",
    "- High-level abstractions (RDDs, DataFrames, Datasets) simplify complex data transformations.\n",
    "\n",
    "### 5. Ecosystem and Integration\n",
    "- Integrates with Hadoop HDFS, Amazon S3, Apache Kafka, and other platforms.\n",
    "- Supports multiple cluster managers for flexible deployment.\n",
    "\n",
    "### 6. Essential for Modern Workloads\n",
    "- Enables ETL pipelines, real-time dashboards, machine learning workflows, and large-scale interactive queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4bbb3f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Typical Use Cases\n",
    "- ETL pipelines for big data ingestion and transformation\n",
    "- Scalable machine learning model training and deployment\n",
    "- Real-time data stream processing (e.g., fraud detection, log analysis)\n",
    "- Graph analytics for social network analysis and recommendations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2349f9c1",
   "metadata": {},
   "source": [
    "# RDDs and DataFrames in Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e0a8fd",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Apache Spark has two core abstractions for working with distributed data:\n",
    "- **RDD (Resilient Distributed Dataset):** The original low-level distributed data structure\n",
    "- **DataFrame:** A high-level abstraction built on top of RDDs, offering a tabular data structure similar to a database table or Pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb91f818",
   "metadata": {},
   "source": [
    "## 2. RDD: Resilient Distributed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ca31a",
   "metadata": {},
   "source": [
    "### 2.1 What is an RDD?\n",
    "An RDD is an immutable distributed collection of objects that can be processed in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436dbb0d",
   "metadata": {},
   "source": [
    "### 2.2 Key Features\n",
    "- Fault-tolerant\n",
    "- Lazy evaluation\n",
    "- Supports transformations (`map`, `filter`, etc.) and actions (`collect`, `count`, etc.)\n",
    "- Type-safe (in Scala/Java)\n",
    "- No built-in schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d32c14",
   "metadata": {},
   "source": [
    "### 2.3 Creating or Loading Data into an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e083a0",
   "metadata": {},
   "source": [
    "#### Creating an RDD (PySpark):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d007ef",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RDDExample\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27779b4",
   "metadata": {},
   "source": [
    "#### Loading Data into an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d5532",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Load file (skip header)\n",
    "rdd = sc.textFile(\"./data/customers.csv\")\n",
    "header = rdd.first()\n",
    "rdd_data = rdd.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f754c6",
   "metadata": {},
   "source": [
    "### 2.4 RDD Transformation and Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe76def",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Split CSV into fields\n",
    "customers_rdd = rdd_data.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc83956",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# View Sample\n",
    "customers_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db71fa6",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Count missing join dates\n",
    "missing_dates = customers_rdd.filter(lambda x: x[4] == \"\").count()\n",
    "print(f\"Missing join dates: {missing_dates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aff271",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Extract customer names\n",
    "names = customers_rdd.map(lambda x: f\"{x[1]} {x[2]}\").collect()\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dc97ba",
   "metadata": {},
   "source": [
    "## 3. DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cadf99",
   "metadata": {},
   "source": [
    "### 3.1 What is a DataFrame?\n",
    "A DataFrame is a distributed collection of data organized into named columns, like a SQL table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfed20cd",
   "metadata": {},
   "source": [
    "### 3.2 Key Features\n",
    "- Schema-aware (columns and types)\n",
    "- Optimized by Catalyst optimizer\n",
    "- Supports SQL queries via `spark.sql()`\n",
    "- Interoperable with RDDs and Pandas\n",
    "- Better performance than RDD for most use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3fc475",
   "metadata": {},
   "source": [
    "### 3.3 Creating or Loading Data into a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdca95d3",
   "metadata": {},
   "source": [
    "#### Reading CSV into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3437db6",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", True).csv(\"./data/customers.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecec80e",
   "metadata": {},
   "source": [
    "### 3.4 Common DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad07543",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Print schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75751ac9",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "df.select(\"first_name\", \"email\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce9452",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Filter customers with missing join dates\n",
    "df.filter(df.join_date.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e98f4",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Count customers who joined\n",
    "df.filter(df.join_date.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4dad45",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Extract customer names (from RDD)\n",
    "names = customers_rdd.map(lambda x: f\"{x[1]} {x[2]}\").collect()\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1864bd1d",
   "metadata": {},
   "source": [
    "## 4. Conversion Between RDD and DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290daf6f",
   "metadata": {},
   "source": [
    "### From RDD to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea8354a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Convert RDD to Row RDD\n",
    "row_rdd = customers_rdd.map(lambda x: Row(\n",
    "    customer_id=int(x[0]),\n",
    "    first_name=x[1],\n",
    "    last_name=x[2],\n",
    "    email=x[3],\n",
    "    join_date=x[4] if x[4] != \"\" else None\n",
    "))\n",
    "\n",
    "df_from_rdd = spark.createDataFrame(row_rdd)\n",
    "df_from_rdd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a1415",
   "metadata": {},
   "source": [
    "### From DataFrame to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6991d0c7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "rdd_from_df = df.rdd\n",
    "rdd_from_df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e504791b",
   "metadata": {},
   "source": [
    "## 5. RDD vs. DataFrame - Comparison\n",
    "\n",
    "| Feature           | RDD                        | DataFrame               |\n",
    "| ----------------- | -------------------------- | ----------------------- |\n",
    "| Abstraction Level | Low                        | High                    |\n",
    "| API Style         | Functional                 | SQL-like                |\n",
    "| Schema            | Not enforced               | Schema-aware            |\n",
    "| Performance       | Lower                      | Optimized with Catalyst |\n",
    "| Best for          | Custom, fine-grained logic | Queries, aggregations   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7311addd",
   "metadata": {},
   "source": [
    "## 6. Use Case Summary\n",
    "\n",
    "| Task                                     | Recommended |\n",
    "| ---------------------------------------- | ----------- |\n",
    "| Load structured CSV data                 | DataFrame   |\n",
    "| Filter or select fields efficiently      | DataFrame   |\n",
    "| Custom parsing, transformation, or logic | RDD         |\n",
    "| SQL-like querying and grouping           | DataFrame   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aa747f",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "- Use DataFrames when working with structured data like CSV, JSON, or Parquet.\n",
    "- Use RDDs when you need custom logic, performance tuning, or low-level transformations.\n",
    "\n",
    "This practical section using your `customers.csv` helps you clearly see how both abstractions work and when to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edba985",
   "metadata": {},
   "source": [
    "# Apache Spark Local Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bfec0c",
   "metadata": {},
   "source": [
    "In this section, we'll cover two common ways to set up Apache Spark on a local development machine:\n",
    "\n",
    "1. **Installing Spark Locally (Native Installation)**\n",
    "2. **Using Docker to Set Up Spark**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72da0ab",
   "metadata": {},
   "source": [
    "## 1. Installing Spark Locally (Native Installation)\n",
    "\n",
    "This method involves manually installing Spark and its dependencies on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f341eaa",
   "metadata": {},
   "source": [
    "### 1.1 Prerequisites\n",
    "- **Java (JDK 8 or 11):** Spark runs on the JVM.\n",
    "- **Python 3.x:** Required for PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e772461",
   "metadata": {},
   "source": [
    "### 1.2 Download and Install Spark\n",
    "- Download Spark from the [Official Apache Spark website](https://spark.apache.org/downloads.html).\n",
    "    - Choose a version (e.g., Spark 3.4.1) and a pre-built package for Hadoop (e.g., \"Pre-built for Apache Hadoop 3.3 and later\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ada4129",
   "metadata": {},
   "source": [
    "#### Extract the archive to a directory of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb00c24",
   "metadata": {},
   "source": [
    "**On Linux:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf77d8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "tar -xzf spark-3.4.1-bin-hadoop3.tgz -C /path/to/your/directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682232d9",
   "metadata": {},
   "source": [
    "**On Windows:**\n",
    "- Use a tool like 7-Zip or WinRAR.\n",
    "    - Right-click the downloaded `.tgz` file\n",
    "    - Select \"Extract Here\" or \"Extract to spark-3.4.1-bin-hadoop3\"\n",
    "    - Move the extracted folder to your desired location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72f571e",
   "metadata": {},
   "source": [
    "### 1.3 Set Environment Variables\n",
    "Set the following environment variables so your system can find Spark and Java."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3817875d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Linux (add to ~/.bashrc or ~/.zshrc)\n",
    "export SPARK_HOME=/path/to/your/directory/spark-3.4.1-bin-hadoop3\n",
    "export PATH=$PATH:$SPARK_HOME/bin\n",
    "export JAVA_HOME=/path/to/your/java"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc798ab",
   "metadata": {},
   "source": [
    "On Windows, set environment variables via System Properties > Environment Variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861d7b5b",
   "metadata": {},
   "source": [
    "### 1.4 Install Required Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc820efe",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pyspark findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7f9047",
   "metadata": {},
   "source": [
    "### 1.5 Test Your Installation\n",
    "Start the PySpark shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36864dfd",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01858e",
   "metadata": {},
   "source": [
    "Or test with a small script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfcb1b1",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Test\").getOrCreate()\n",
    "print(spark.range(5).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4f58a2",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Using Docker to Set Up Spark\n",
    "\n",
    "An alternative way is to run Spark inside Docker containers. This avoids manual setup and ensures a clean environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56a1f10",
   "metadata": {},
   "source": [
    "### 2.1 Prerequisites\n",
    "- Docker installed on your system ([Install Docker](https://docs.docker.com/get-docker/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595655e4",
   "metadata": {},
   "source": [
    "### 2.2 Standalone Setup\n",
    "#### 2.2.1 Pull a Spark Docker image\n",
    "You can use an existing image from Docker Hub or customize it using a Dockerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b834f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker pull bitnami/spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee2a214",
   "metadata": {},
   "source": [
    "#### 2.2.2 Run a Spark Container\n",
    "Start a Spark standalone container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c4128d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run -it bitnami/spark pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d1204d",
   "metadata": {},
   "source": [
    "### 2.3 Set Up Spark Cluster\n",
    "You can create a local Spark cluster with [`docker-compose.yaml`](./docker-compose.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7577eceb",
   "metadata": {},
   "source": [
    "#### 2.3.1 Start the Cluster\n",
    "Run the following command to start the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1b31f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820a333",
   "metadata": {},
   "source": [
    "#### 2.3.2 Access the Spark Web UI\n",
    "- Master: [http://localhost:8080](http://localhost:8080)\n",
    "- Worker: [http://localhost:8081](http://localhost:8081)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88072a97",
   "metadata": {},
   "source": [
    "#### 2.3.3 Submit Jobs\n",
    "You can submit jobs using the spark-submit tool or run a PySpark shell inside the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af7797",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker exec -it spark-master pyspark --master spark://spark-master:7077"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd69293",
   "metadata": {},
   "source": [
    "#### 2.3.4 Setting Up Jupyter Notebook Container for Spark (Optional)\n",
    "\n",
    "Running a Jupyter Notebook container alongside your Spark services is a great way to interactively test Spark code using PySpark.\n",
    "\n",
    "- Uncomment the `jupyter` service block in the [`docker-compose.yaml`](./docker-compose.yaml) file.\n",
    "- Ensure the `notebooks` directory exists in the same location as your `docker-compose.yaml`:\n",
    "    ```bash\n",
    "    mkdir notebooks\n",
    "    ```\n",
    "  This directory will be mounted into the Jupyter container so that your notebooks are saved persistently.\n",
    "\n",
    "- To start the whole cluster (including Jupyter):\n",
    "    ```bash\n",
    "    docker-compose up -d\n",
    "    ```\n",
    "- To start only the Jupyter container (after cluster is running):\n",
    "    ```bash\n",
    "    docker-compose up -d jupyter\n",
    "    ```\n",
    "- You can now access the notebook UI at: [http://localhost:8888](http://localhost:8888)\n",
    "\n",
    "  Use the token shown in the terminal (when the Jupyter container starts) to log in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a89c5",
   "metadata": {},
   "source": [
    "#### 2.3.5 Test Notebook Code\n",
    "\n",
    "In a new notebook, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e990c0d9",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NotebookSpark\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.range(5).show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
