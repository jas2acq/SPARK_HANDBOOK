{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "943881ad",
   "metadata": {},
   "source": [
    "# Spark Handbook\n",
    "## Apache Spark: A Comprehensive Guide for Data Engineers\n",
    "This handbook provides a comprehensive overview of [Apache Spark](glossary.md#apache-spark), a powerful [distributed data processing framework](glossary.md#distributed-data-processing-framework) designed for handling [big data](glossary.md#big-data) workloads with speed, ease of use, and flexibility.\n",
    "\n",
    "## Table of Contents\n",
    "- [How Apache Spark Works](#how-apache-spark-works)\n",
    "- [Apache Spark Architecture](#apache-spark-architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc1d7ef",
   "metadata": {},
   "source": [
    "## What is Apache Spark?\n",
    "\n",
    "**Apache Spark** is an open-source, distributed analytics engine designed for large-scale data processing and machine learning. It is renowned for its speed, versatility, and ability to scale from a single machine to large clusters of computers. Spark offers APIs in several popular languages, including Python (using PySpark), Scala, Java, and R, making it accessible to a wide audience of data professionals.\n",
    "\n",
    "## Main Abstractions of Apache Spark\n",
    "\n",
    "### Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "RDDs are the fundamental data structure in Spark. They represent immutable, distributed collections of objects partitioned across the cluster. RDDs support two types of operations:\n",
    "\n",
    "- **Transformations:** Lazy operations that define a new RDD from an existing one (e.g., map, filter).\n",
    "- **Actions:** Operations that trigger computation and return results (e.g., collect, count).\n",
    "\n",
    "RDDs enable fault tolerance by tracking lineage, allowing the system to recompute lost data partitions in case of node failure.\n",
    "\n",
    "### Directed Acyclic Graph (DAG)\n",
    "\n",
    "Spark uses a DAG to represent the sequence of transformations applied to RDDs. When a job is submitted, Spark’s DAG scheduler breaks the computation into stages of tasks that can be executed in parallel. This DAG-based execution plan enables optimization and efficient job scheduling.\n",
    "\n",
    "## Spark Architecture\n",
    "\n",
    "Apache Spark follows a **master-worker** architecture composed of several key components:\n",
    "\n",
    "### 1. Spark Driver\n",
    "\n",
    "The driver is the central coordinator of a Spark application. It:\n",
    "\n",
    "- Maintains the lifecycle of the application.\n",
    "- Converts user code into a logical execution plan (DAG).\n",
    "- Schedules tasks and monitors their execution.\n",
    "- Communicates with the cluster manager to request resources.\n",
    "- Collects and aggregates results from executors.\n",
    "\n",
    "The driver contains components such as SparkContext, DAG Scheduler, Task Scheduler, and Block Manager.\n",
    "\n",
    "### 2. Cluster Manager\n",
    "\n",
    "The cluster manager oversees resource allocation in the cluster. It manages CPUs, memory, and executors across worker nodes. Spark can run on various cluster managers such as:\n",
    "\n",
    "- Apache YARN (Hadoop ecosystem)\n",
    "- Apache Mesos\n",
    "- Kubernetes\n",
    "- Spark's standalone cluster manager\n",
    "\n",
    "### 3. Executors\n",
    "\n",
    "Executors are worker processes launched on cluster nodes. Each executor:\n",
    "\n",
    "- Executes tasks assigned by the driver.\n",
    "- Performs computations on partitions of data.\n",
    "- Caches data in memory or on disk as needed.\n",
    "- Reports task status and results back to the driver.\n",
    "\n",
    "Executors live for the duration of a Spark application and enable parallel task execution.\n",
    "\n",
    "### 4. Worker Nodes\n",
    "\n",
    "Worker nodes are the physical or virtual machines in the cluster where executors run. They host one or more executors executing tasks in parallel.\n",
    "\n",
    "### 5. SparkContext\n",
    "\n",
    "SparkContext is the entry point through which the driver interacts with the cluster. It:\n",
    "\n",
    "- Connects to the cluster manager.\n",
    "- Creates RDDs and manages their lifecycle.\n",
    "- Coordinates job execution.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## Spark Core Components and Libraries\n",
    "\n",
    "### Spark SQL\n",
    "\n",
    "Spark SQL is Spark’s module for working with structured data. It allows querying data using:\n",
    "\n",
    "- Standard SQL.\n",
    "- Hive Query Language (HQL).\n",
    "- Support for numerous data sources including Hive tables, Parquet, and JSON.\n",
    "\n",
    "Spark SQL integrates SQL queries with Spark’s programmatic APIs (RDDs, DataFrames) in Python, Scala, and Java. This tight integration supports complex analytics and interactive querying within a unified application framework.\n",
    "\n",
    "Spark SQL replaced older projects like Shark (an earlier SQL-on-Spark project from UC Berkeley) to offer better compatibility and performance within the Spark ecosystem.\n",
    "\n",
    "### MLlib\n",
    "\n",
    "MLlib is Spark’s scalable machine learning library. It provides:\n",
    "\n",
    "- Algorithms for classification, regression, clustering, and collaborative filtering.\n",
    "- Utilities for model evaluation and data import.\n",
    "- Low-level primitives such as a generic gradient descent optimization algorithm.\n",
    "\n",
    "MLlib is designed for distributed processing, enabling large-scale machine learning tasks across clusters.\n",
    "\n",
    "### GraphX\n",
    "\n",
    "GraphX is Spark’s graph processing library, enabling:\n",
    "\n",
    "- Creation and manipulation of graphs with properties on vertices and edges.\n",
    "- Graph-parallel computations like PageRank and triangle counting.\n",
    "- Operators such as subgraph extraction and vertex mapping.\n",
    "\n",
    "GraphX extends the Spark RDD API, making graph analytics a natural part of Spark’s unified data processing framework.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240422bd",
   "metadata": {},
   "source": [
    "\n",
    "## Why Do Data Engineers Need Spark?\n",
    "\n",
    "### 1. Speed and Performance\n",
    "\n",
    "- Spark performs in-memory computing, reducing costly disk read/write operations.\n",
    "- It can be up to 100× faster than Hadoop MapReduce for iterative and interactive workloads.\n",
    "\n",
    "### 2. Scalability\n",
    "\n",
    "- Spark scales from a single machine to thousands of cluster nodes.\n",
    "- Handles petabyte-scale data through distributed processing.\n",
    "\n",
    "### 3. Unified Processing Engine\n",
    "\n",
    "- Supports batch processing, real-time streaming, SQL querying, machine learning, and graph analytics all within one platform.\n",
    "\n",
    "### 4. Language Flexibility and Ease of Use\n",
    "\n",
    "- Provides APIs in Python, Scala, Java, and R.\n",
    "- High-level abstractions (RDDs, DataFrames, Datasets) simplify complex data transformations.\n",
    "\n",
    "### 5. Ecosystem and Integration\n",
    "\n",
    "- Integrates with Hadoop HDFS, Amazon S3, Apache Kafka, and other platforms.\n",
    "- Supports multiple cluster managers for flexible deployment.\n",
    "\n",
    "### 6. Essential for Modern Workloads\n",
    "\n",
    "- Enables ETL pipelines, real-time dashboards, machine learning workflows, and large-scale interactive queries.\n",
    "\n",
    "---\n",
    "\n",
    "## Typical Use Cases\n",
    "\n",
    "- ETL pipelines for big data ingestion and transformation\n",
    "- Scalable machine learning model training and deployment\n",
    "- Real-time data stream processing (e.g., fraud detection, log analysis)\n",
    "- Graph analytics for social network analysis and recommendations\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cde4db",
   "metadata": {},
   "source": [
    "## How Apache Spark Works\n",
    "\n",
    "![How Spark Works](img/how_spark_works.png)\n",
    "\n",
    "[Apache Spark](glossary.md#apache-spark) is a [distributed data processing framework](glossary.md#distributed-data-processing-framework) designed to handle [big data](glossary.md#big-data) workloads with speed, ease of use, and flexibility. The fundamental principle behind Spark's operation is its [master-slave architecture](glossary.md#master-slave-architecture), which allows it to execute tasks in parallel across a cluster of machines.\n",
    "\n",
    "### Key Concepts in Spark’s Operation\n",
    "\n",
    "#### Driver Program\n",
    "The [driver](glossary.md#driver) is the central coordinator and controller of a Spark application. When you start a Spark application, the driver runs your main program. It is responsible for:\n",
    "- Creating a [SparkContext](glossary.md#sparkcontext), which is the entry point to all Spark functionalities.\n",
    "- Converting the user’s code (written in Scala, Python, Java, or R) into a [logical execution plan](glossary.md#logical-execution-plan).\n",
    "- Breaking down the application into smaller pieces called [jobs](glossary.md#jobs) and subsequently into [stages](glossary.md#stages) and [tasks](glossary.md#tasks).\n",
    "- Scheduling tasks on [executors](glossary.md#executors) and managing their lifecycle.\n",
    "- Handling [fault tolerance](glossary.md#fault-tolerance) by retrying failed tasks and reallocating resources.\n",
    "\n",
    "#### SparkContext\n",
    "[SparkContext](glossary.md#sparkcontext) represents the connection to the computing cluster. It acts as the interface between your Spark application and the [cluster manager](glossary.md#cluster-manager), letting your program create [distributed collections](glossary.md#distributed-collections) ([RDDs](glossary.md#rdds) — Resilient Distributed Datasets), [accumulators](glossary.md#accumulators), and [broadcast variables](glossary.md#broadcast-variables).\n",
    "\n",
    "#### Executors\n",
    "[Executors](glossary.md#executors) are worker processes that run on cluster nodes. Each executor:\n",
    "- Receives [tasks](glossary.md#tasks) from the [driver](glossary.md#driver).\n",
    "- Executes those tasks concurrently.\n",
    "- Stores intermediate data and results either in memory or on disk.\n",
    "- Returns results and task status (success or failure) back to the driver.\n",
    "Their lifespan is tied to the lifecycle of the Spark application.\n",
    "\n",
    "#### Cluster Manager\n",
    "The [cluster manager](glossary.md#cluster-manager) is a separate system responsible for managing cluster resources and allocating them to various applications. Spark can operate with several cluster managers:\n",
    "- [Standalone cluster manager](glossary.md#standalone-cluster-manager) (provided by Spark itself for simple setups).\n",
    "- [Apache Mesos](glossary.md#apache-mesos) (a general-purpose cluster manager).\n",
    "- [Hadoop YARN](glossary.md#hadoop-yarn) (resource manager used with Hadoop clusters).\n",
    "- [Kubernetes](glossary.md#kubernetes) (for container orchestration).\n",
    "\n",
    "The cluster manager launches the Spark [driver](glossary.md#driver) and [executors](glossary.md#executors) on cluster nodes, depending on the execution mode.\n",
    "\n",
    "### Spark’s Execution Workflow\n",
    "1. When an application starts, the [driver program](glossary.md#driver) is launched, which creates the [SparkContext](glossary.md#sparkcontext).\n",
    "2. The driver creates a [Directed Acyclic Graph (DAG)](glossary.md#dag) representing the computation flow based on user operations ([transformations](glossary.md#transformations) and [actions](glossary.md#actions)).\n",
    "3. The [DAG Scheduler](glossary.md#dag-scheduler) breaks this DAG into [stages](glossary.md#stages), grouping tasks based on [shuffle boundaries](glossary.md#shuffle-boundaries) and data dependencies.\n",
    "4. The [Task Scheduler](glossary.md#task-scheduler) then schedules individual [tasks](glossary.md#tasks) within the stages for execution on the [executors](glossary.md#executors).\n",
    "5. Tasks are assigned to executors running on worker nodes.\n",
    "6. Executors perform computations, cache data as needed (to speed up repeated data processing), and report results and status back to the driver.\n",
    "7. The driver aggregates results and completes the [job](glossary.md#jobs).\n",
    "\n",
    "### Performance Optimizations in Spark\n",
    "- [In-Memory Computation](glossary.md#in-memory-computation): Spark loads data into memory and performs computations there, minimizing slower disk I/O operations.\n",
    "- [Data Caching](glossary.md#data-caching): Frequently used datasets can be cached in memory across iterations to enhance performance, particularly useful for [machine learning](glossary.md#machine-learning) and iterative algorithms.\n",
    "- [Stage Pipelining](glossary.md#stage-pipelining): Multiple operations can be pipelined within a stage if they do not require a shuffle, avoiding unnecessary disk writes.\n",
    "- [Fault Tolerance](glossary.md#fault-tolerance): Spark maintains [lineage information](glossary.md#lineage-information) of [RDDs](glossary.md#rdds), so it knows how to recompute lost data partitions in case of executor failures.\n",
    "\n",
    "### Spark Workloads and Ecosystem Components\n",
    "Spark is not just a batch processing engine but a full ecosystem for diverse workloads:\n",
    "- [Spark Core](glossary.md#spark-core): Handles basic operations like [job scheduling](glossary.md#job-scheduling), memory management, [fault recovery](glossary.md#fault-recovery), and task dispatching.\n",
    "- [Spark SQL](glossary.md#spark-sql): Provides interactive querying capabilities using SQL or Hive Query Language with high-performance engines.\n",
    "- [Spark Streaming](glossary.md#spark-streaming): Enables real-time data processing through micro-batch streaming of live data sources such as [Kafka](glossary.md#kafka) and Twitter.\n",
    "- [MLlib](glossary.md#mllib): Spark’s machine learning library providing scalable algorithms including classification, regression, clustering, and collaborative filtering.\n",
    "- [GraphX](glossary.md#graphx): Framework for graph processing and computation across [distributed datasets](glossary.md#distributed-datasets).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c81f810",
   "metadata": {},
   "source": [
    "## Apache Spark Architecture\n",
    "\n",
    "![Spark Architecture](img/spark_architecture.png)\n",
    "\n",
    "[Apache Spark](glossary.md#apache-spark)’s architecture comprises several key components and follows a modular, layered design optimized for [distributed processing](glossary.md#distributed-processing).\n",
    "\n",
    "### 1. Driver Program (Master Node)\n",
    "- Runs your application containing the user’s code.\n",
    "- Manages [SparkContext](glossary.md#sparkcontext) and coordinates the execution of [tasks](glossary.md#tasks).\n",
    "- Converts user operations into a [DAG](glossary.md#dag).\n",
    "- Interacts with the [cluster manager](glossary.md#cluster-manager) for resource allocation.\n",
    "- Oversees [job scheduling](glossary.md#job-scheduling) via the [DAG Scheduler](glossary.md#dag-scheduler) and [Task Scheduler](glossary.md#task-scheduler).\n",
    "- Maintains cluster state and tracks job progress and [fault handling](glossary.md#fault-handling).\n",
    "\n",
    "The [driver](glossary.md#driver) is critical because it manages job orchestration and monitors system health and task execution.\n",
    "\n",
    "### 2. Cluster Manager\n",
    "- A standalone service or integration with other cluster management tools.\n",
    "- Manages resources across the cluster.\n",
    "- Launches the [driver](glossary.md#driver) and [executor](glossary.md#executors) processes as per the requested resources.\n",
    "- Monitors node health and manages failures within the cluster.\n",
    "\n",
    "Supported cluster managers include:\n",
    "- [Spark Standalone](glossary.md#standalone-cluster-manager) (simple and easy setup).\n",
    "- [Apache Mesos](glossary.md#apache-mesos) (multi-framework support).\n",
    "- [Hadoop YARN](glossary.md#hadoop-yarn) (common in Hadoop ecosystems).\n",
    "- [Kubernetes](glossary.md#kubernetes) (for containerized Spark deployments).\n",
    "\n",
    "### 3. Executors (Worker Nodes)\n",
    "- Executor processes run on each worker node in the cluster.\n",
    "- Perform actual data processing by executing [tasks](glossary.md#tasks) assigned by the [driver](glossary.md#driver).\n",
    "- Cache data in memory or on disk for efficient reuse.\n",
    "- Handle communication with the driver, sending back task execution results.\n",
    "- Their number can be configured based on workload and cluster size.\n",
    "\n",
    "### 4. Resilient Distributed Dataset (RDD) - The Core Abstraction\n",
    "- [RDDs](glossary.md#rdds) are immutable distributed collections of objects partitioned across the cluster.\n",
    "- They provide [fault tolerance](glossary.md#fault-tolerance) by logging [lineage information](glossary.md#lineage-information), enabling automatic recomputation.\n",
    "- Users can perform [transformations](glossary.md#transformations) (lazy evaluated) and [actions](glossary.md#actions) on RDDs.\n",
    "- RDD abstractions facilitate parallel computations without explicit data movement handling.\n",
    "\n",
    "### 5. Directed Acyclic Graph (DAG)\n",
    "- The [DAG](glossary.md#dag) abstraction represents [stages](glossary.md#stages) and [tasks](glossary.md#tasks) of computation.\n",
    "- Directed graph with no cycles that represents the dependencies between [transformations](glossary.md#transformations).\n",
    "- The [DAG Scheduler](glossary.md#dag-scheduler) converts the program's DAG into stages for execution optimization.\n",
    "- Enables [pipeline execution](glossary.md#pipeline-execution) within stages and minimizes overhead of disk I/O.\n",
    "\n",
    "### Execution Modes in Spark\n",
    "Spark supports three main modes of execution which influence where the [driver](glossary.md#driver) and [executors](glossary.md#executors) run:\n",
    "\n",
    "#### Cluster Mode\n",
    "- [Driver](glossary.md#driver) runs inside the cluster on one of the worker nodes.\n",
    "- [Cluster manager](glossary.md#cluster-manager) manages driver and all executor processes.\n",
    "- Suitable for production deployments.\n",
    "\n",
    "#### Client Mode\n",
    "- [Driver](glossary.md#driver) runs on the client machine from which the job was submitted.\n",
    "- [Executors](glossary.md#executors) run on the cluster nodes.\n",
    "- Useful for interactive debugging or testing.\n",
    "\n",
    "#### Local Mode\n",
    "- Entire Spark application executes on a single machine.\n",
    "- Parallelism is achieved using multiple threads.\n",
    "- Mostly used for development, experimentation, and debugging.\n",
    "- Not recommended for production jobs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d0dc00",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
