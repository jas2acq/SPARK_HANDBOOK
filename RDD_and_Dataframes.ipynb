{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6ba173",
   "metadata": {},
   "source": [
    "# RDDs and DataFrames in Apache Spark\n",
    "\n",
    "Dataset: `./data/customers.csv`\n",
    "\n",
    "## Table of Contents\n",
    "- [1. Introduction](#1-introduction)\n",
    "- [2. RDD: Resilient Distributed Dataset](#2-rdd-resilient-distributed-dataset)\n",
    "  - [2.1 What is an RDD?](#21-what-is-an-rdd)\n",
    "  - [2.2 Key Features](#22-key-features)\n",
    "  - [2.3 Creating or Loading Data into an RDD](#23-creating-or-loading-data-into-an-rdd)\n",
    "  - [2.4 RDD Transformation and Actions](#24-rdd-transformation-and-actions)\n",
    "- [3. DataFrames](#3-dataframes)\n",
    "  - [3.1 What is a DataFrame?](#31-what-is-a-dataframe)\n",
    "  - [3.2 Key Features](#32-key-features)\n",
    "  - [3.3 Creating or Loading Data into a DataFrame](#33-creating-or-loading-data-into-a-dataframe)\n",
    "  - [3.4 Common DataFrame Operations](#34-common-dataframe-operations)\n",
    "- [4. Conversion Between RDD and DataFrame](#4-conversion-between-rdd-and-dataframe)\n",
    "- [5. RDD vs. DataFrame - Comparison](#5-rdd-vs-dataframe---comparison)\n",
    "- [6. Use Case Summary](#6-use-case-summary)\n",
    "- [7. Conclusion](#7-conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd36589",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "Apache Spark has two core abstractions for working with distributed data:\n",
    "- **RDD (Resilient Distributed Dataset):** The original low-level distributed data structure\n",
    "- **DataFrame:** A high-level abstraction built on top of RDDs, offering a tabular data structure similar to a database table or Pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338e4622",
   "metadata": {},
   "source": [
    "## 2. RDD: Resilient Distributed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379a33c2",
   "metadata": {},
   "source": [
    "### 2.1 What is an RDD?\n",
    "An RDD is an immutable distributed collection of objects that can be processed in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2a7c4e",
   "metadata": {},
   "source": [
    "### 2.2 Key Features\n",
    "- Fault-tolerant\n",
    "- Lazy evaluation\n",
    "- Supports transformations (`map`, `filter`, etc.) and actions (`collect`, `count`, etc.)\n",
    "- Type-safe (in Scala/Java)\n",
    "- No built-in schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c38522",
   "metadata": {},
   "source": [
    "### 2.3 Creating or Loading Data into an RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4644da69",
   "metadata": {},
   "source": [
    "#### Creating an RDD (PySpark):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a68fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RDDExample\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de20d229",
   "metadata": {},
   "source": [
    "#### Loading Data into an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68f9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file (skip header)\n",
    "rdd = sc.textFile(\"./data/customers.csv\")\n",
    "header = rdd.first()\n",
    "rdd_data = rdd.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8616c3f4",
   "metadata": {},
   "source": [
    "### 2.4 RDD Transformation and Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6735d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split CSV into fields\n",
    "customers_rdd = rdd_data.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db8450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Sample\n",
    "customers_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c37f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing join dates\n",
    "missing_dates = customers_rdd.filter(lambda x: x[4] == \"\").count()\n",
    "print(f\"Missing join dates: {missing_dates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract customer names\n",
    "names = customers_rdd.map(lambda x: f\"{x[1]} {x[2]}\").collect()\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38602126",
   "metadata": {},
   "source": [
    "## 3. DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af9b3d",
   "metadata": {},
   "source": [
    "### 3.1 What is a DataFrame?\n",
    "A DataFrame is a distributed collection of data organized into named columns, like a SQL table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae2661a",
   "metadata": {},
   "source": [
    "### 3.2 Key Features\n",
    "- Schema-aware (columns and types)\n",
    "- Optimized by Catalyst optimizer\n",
    "- Supports SQL queries via `spark.sql()`\n",
    "- Interoperable with RDDs and Pandas\n",
    "- Better performance than RDD for most use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800014ff",
   "metadata": {},
   "source": [
    "### 3.3 Creating or Loading Data into a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607cfcd3",
   "metadata": {},
   "source": [
    "#### Reading CSV into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c728e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", True).csv(\"./data/customers.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb7835",
   "metadata": {},
   "source": [
    "### 3.4 Common DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556184dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a6a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "df.select(\"first_name\", \"email\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d3269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter customers with missing join dates\n",
    "df.filter(df.join_date.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c47f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count customers who joined\n",
    "df.filter(df.join_date.isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a7b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract customer names (from RDD)\n",
    "names = customers_rdd.map(lambda x: f\"{x[1]} {x[2]}\").collect()\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de71bba1",
   "metadata": {},
   "source": [
    "## 4. Conversion Between RDD and DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d3fcb",
   "metadata": {},
   "source": [
    "### From RDD to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa6929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Convert RDD to Row RDD\n",
    "row_rdd = customers_rdd.map(lambda x: Row(\n",
    "    customer_id=int(x[0]),\n",
    "    first_name=x[1],\n",
    "    last_name=x[2],\n",
    "    email=x[3],\n",
    "    join_date=x[4] if x[4] != \"\" else None\n",
    "))\n",
    "\n",
    "df_from_rdd = spark.createDataFrame(row_rdd)\n",
    "df_from_rdd.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a499c",
   "metadata": {},
   "source": [
    "### From DataFrame to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4501daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_from_df = df.rdd\n",
    "rdd_from_df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677a6a13",
   "metadata": {},
   "source": [
    "## 5. RDD vs. DataFrame - Comparison\n",
    "\n",
    "| Feature           | RDD                        | DataFrame               |\n",
    "| ----------------- | -------------------------- | ----------------------- |\n",
    "| Abstraction Level | Low                        | High                    |\n",
    "| API Style         | Functional                 | SQL-like                |\n",
    "| Schema            | Not enforced               | Schema-aware            |\n",
    "| Performance       | Lower                      | Optimized with Catalyst |\n",
    "| Best for          | Custom, fine-grained logic | Queries, aggregations   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d3a61",
   "metadata": {},
   "source": [
    "## 6. Use Case Summary\n",
    "\n",
    "| Task                                     | Recommended |\n",
    "| ---------------------------------------- | ----------- |\n",
    "| Load structured CSV data                 | DataFrame   |\n",
    "| Filter or select fields efficiently      | DataFrame   |\n",
    "| Custom parsing, transformation, or logic | RDD         |\n",
    "| SQL-like querying and grouping           | DataFrame   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c4c8b",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "- Use DataFrames when working with structured data like CSV, JSON, or Parquet.\n",
    "- Use RDDs when you need custom logic, performance tuning, or low-level transformations.\n",
    "\n",
    "This practical section using your `customers.csv` helps you clearly see how both abstractions work and when to use them."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
